{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "import pandas as pd         # for loadData()\n",
    "import open3d as o3d        # for getting point cloud register\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm       # for showing progress when training\n",
    "#test update for branch wild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperation (NOTE: Using meter as unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "# convert pointcloud from cartisean coordinate to spherical coordinate\n",
    "def cart2sph(xyz):\n",
    "    x = xyz[:,0]\n",
    "    y = xyz[:,1]\n",
    "    z = xyz[:,2]\n",
    "    XsqPlusYsq = x**2 + y**2\n",
    "    r = np.sqrt(list(XsqPlusYsq + z**2))\n",
    "    elev = np.arctan2(list(z), np.sqrt(list(XsqPlusYsq)))\n",
    "    pan = np.arctan2(list(y), list(x))\n",
    "    output = np.array([r, elev, pan])\n",
    "    return rearrange(output, 'a b -> b a') #take transpose\n",
    "\n",
    "def sph2cart(ang):\n",
    "    ele = ang[:,0]\n",
    "    pan = ang[:,1]\n",
    "    x = np.cos(ele)*np.cos(pan)\n",
    "    y = np.cos(ele)*np.sin(pan)\n",
    "    z = np.sin(ele)\n",
    "    output = np.array([x,y,z])\n",
    "    return rearrange(output, 'a b -> b a') #take transpose\n",
    "\n",
    "def cart2sph_tensor(coords):\n",
    "    x, y, z = coords[:, 0], coords[:, 1], coords[:, 2]\n",
    "    r = torch.sqrt(x**2 + y**2 + z**2)\n",
    "    theta = torch.acos(z / r)\n",
    "    phi = torch.atan2(y, x)\n",
    "    return torch.stack([r, theta, phi], dim=1)\n",
    "\n",
    "def wrapping(coords, radius):\n",
    "    x, y, z = coords[:,0], coords[:,1], coords[:,2]\n",
    "    l = x**2 + y**2 + z**2 \n",
    "    less_than_r = l <= radius\n",
    "    more_than_r = l >= radius\n",
    "    pts_in_sphere = coords[less_than_r]\n",
    "    pts_out_sphere = coords[more_than_r]\n",
    "    len_out_sphere = l[more_than_r]\n",
    "    wrapped = (2*radius - 1/len_out_sphere)*(pts_out_sphere / len_out_sphere)\n",
    "    return pts_in_sphere + wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(dataset_path):\n",
    "    # List all files in the specified path, ignoring directories\n",
    "    files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "    files.sort()\n",
    "    # read the files\n",
    "    points_xyz = []\n",
    "    for s in files:\n",
    "        path = dataset_path + s\n",
    "        df = pd.read_csv(path)\n",
    "        a = df.to_numpy()\n",
    "        points_xyz.append(a[:,8:11])\n",
    "    return points_xyz\n",
    "\n",
    "def getTransformation(data):\n",
    "    \"\"\" \n",
    "    Accept input of list of numpy array of n*3 size, \n",
    "    return list of 4*4 numpy array of transformation matrix\n",
    "    each are transformation needed from each frame to point cloud in fame 0\n",
    "    \"\"\"\n",
    "    # convert numpy array \n",
    "    point_clouds = []\n",
    "    for d in data:\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(d)\n",
    "        point_clouds.append(pcd)\n",
    "    threshold = 2.0 \n",
    "    trans_init = np.eye(4)  # Initial transformation matrix\n",
    "    transformations = []\n",
    "\n",
    "    # the following may take a while\n",
    "    for i in range(len(point_clouds)):\n",
    "        source_pcd = point_clouds[i]\n",
    "        target_pcd = point_clouds[0]\n",
    "        reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "                    source_pcd, \n",
    "                    target_pcd, \n",
    "                    threshold, \n",
    "                    trans_init,\n",
    "                    o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "        transformations.append(reg_p2p.transformation)\n",
    "    return transformations\n",
    "    \n",
    "def prepareData(points_xyz):\n",
    "    # get transformations\n",
    "    transformations = getTransformation(points_xyz)\n",
    "\n",
    "    # register all points onto same global coordinte (global coordinate align with frame 0 coordinate)\n",
    "    points_reg_xyz = []\n",
    "    for i, points in enumerate(points_xyz):\n",
    "        ones = np.ones((len(points), 1))\n",
    "        homo_points = np.hstack((points, ones))\n",
    "        # apply transformation\n",
    "        t = transformations[i]\n",
    "        t = rearrange(t, 'a b -> b a')\n",
    "        reg_points = homo_points@t\n",
    "        reg_points = reg_points / rearrange(reg_points[:,3], 'a -> a 1')\n",
    "        reg_points = reg_points[:,0:3]\n",
    "        points_reg_xyz.append(reg_points)\n",
    "\n",
    "    # create a list of origins \n",
    "    centres = [(t@np.array([[0],[0],[0],[1]]))[0:3,0] for t in transformations]\n",
    "    \n",
    "    # get the angular direction and distance of rays    \n",
    "    points_sph = []\n",
    "    for i, points in enumerate(points_reg_xyz):\n",
    "        relative_loc = points - centres[i]\n",
    "        points_sph.append(cart2sph(relative_loc))\n",
    "\n",
    "    # tile sensor centre \n",
    "    centres_tiled = []\n",
    "    for i, centre in enumerate(centres):\n",
    "        l = len(points_sph[i])\n",
    "        temp = np.tile(centre, (l, 1))\n",
    "        centres_tiled.append(temp)\n",
    "    \n",
    "    # stack everything into a matrix of size n * 6\n",
    "    # where n is number of points, 6 corrsponds to:\n",
    "    # distance, elevation, pan, x of camera, y of camera, z of camera\n",
    "    # stack the points into one big matrix\n",
    "    stacked = []\n",
    "    for i in range(len(points_sph)):\n",
    "        temp = np.hstack((points_sph[i], centres_tiled[i]))\n",
    "        stacked.append(temp)\n",
    "\n",
    "    dataset = np.array([])\n",
    "    for i in range(len(stacked)):\n",
    "        if i == 0:\n",
    "            dataset = stacked[i]\n",
    "        else:\n",
    "            dataset = np.vstack((dataset, stacked[i]))\n",
    "\n",
    "    # Mid pass filter, for distance value between 2 and 50 meter\n",
    "    mask1 = dataset[:,0] > 2\n",
    "    dataset = dataset[mask1]\n",
    "    mask2 = dataset[:,0] < 50\n",
    "    dataset = dataset[mask2]\n",
    "    np.random.shuffle(dataset)      # shuffle for good practice\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiDAR_NeRF(nn.Module):\n",
    "    def __init__(self, embedding_dim_pos = 10, embedding_dim_dir = 4, hidden_dim = 256, device = 'cuda'):\n",
    "        super(LiDAR_NeRF, self).__init__()\n",
    "        self.device = device\n",
    "        self.embedding_dim_dir = embedding_dim_dir\n",
    "        self.embedding_dim_pos = embedding_dim_pos\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim_pos * 6 + 3 + embedding_dim_dir * 4 + 2, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Sigmoid(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Sigmoid(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Sigmoid(),               \n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim_pos * 6 + 3 + embedding_dim_dir * 4 + 2 + hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Sigmoid(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Sigmoid(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def positional_encoding(x, L):\n",
    "        out = [x]\n",
    "        for j in range(L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def wrapping(coords, wrapping_radius = 10):\n",
    "        x, y, z = coords[:,0], coords[:,1], coords[:,2]\n",
    "        l = torch.sqrt(x**2 + y**2 + z**2)\n",
    "        wrap_factor = (2 - wrapping_radius/l)*(wrapping_radius/l)\n",
    "        wrap_factor[l<=wrapping_radius] = 1\n",
    "        wrap_factor = rearrange(wrap_factor, 'a -> a 1')\n",
    "        print(wrap_factor[0:100])\n",
    "        wrapped_coords = coords * wrap_factor\n",
    "        return wrapped_coords\n",
    "    \n",
    "    def forward(self, o, d):\n",
    "        pos = self.wrapping(o)\n",
    "        emb_x = self.positional_encoding(pos, self.embedding_dim_pos)\n",
    "        emb_d = self.positional_encoding(d, self.embedding_dim_dir)\n",
    "        input = torch.hstack((emb_x,emb_d)).to(dtype=torch.float32)\n",
    "        temp = self.block1(input)\n",
    "        input2 = torch.hstack((temp, input)).to(dtype=torch.float32) # add skip input\n",
    "        output = self.block2(input2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSamples(origins, angles, ground_truth_distance, num_bins = 100):\n",
    "    elev = angles[:,0]\n",
    "    pan = angles[:,1]\n",
    "    dir_x = torch.tensor(np.cos(elev)*np.cos(pan))      # [batch_size]\n",
    "    dir_y = torch.tensor(np.cos(elev)*np.sin(pan))      # [batch_size]\n",
    "    dir_z = torch.tensor(np.sin(elev))\n",
    "    gt_tensor = torch.tensor(ground_truth_distance)\n",
    "\n",
    "    # create a list of magnitudes with even spacing from 0 to 1\n",
    "    t = torch.linspace(0,1, num_bins//2).expand(dir_x.shape[0], num_bins//2)  # [batch_size, num_bins//2]\n",
    "    \n",
    "    # preterb the spacing\n",
    "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "    lower = torch.cat((t[:, :1], mid), -1)\n",
    "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape)\n",
    "    t = lower + (upper - lower) * u  # [batch_size, nb_bins//2]\n",
    "    \n",
    "    # multiply the magnitude to ground truth distance and add 3 meter\n",
    "    t = torch.sqrt(t)\n",
    "    t = torch.sqrt(t)\n",
    "    t2 = 2 - t\n",
    "    t = torch.hstack((t, t2))       #[]\n",
    "    t = rearrange(t, 'a b -> b a')  # [num_bins, batch_size]  transpose for multiplication broadcast\n",
    "    t = gt_tensor*t\n",
    "\n",
    "    # convert magnitudes into positions by multiplying it to the unit vector\n",
    "    pos_x = dir_x*t     # [num_bins, batch_size]\n",
    "    pos_y = dir_y*t\n",
    "    pos_z = dir_z*t\n",
    "    # concat them for output\n",
    "    multiplied = rearrange([pos_x,pos_y,pos_z], 'c b n  -> (n b) c')   # [num_bin*batchsize, 3]\n",
    "    # tile the origin values\n",
    "    origins_tiled = repeat(origins, 'n c -> (n b) c', b = num_bins) # [num_bin*batch_size, 3]\n",
    "    pos = torch.tensor(origins_tiled) + multiplied\n",
    "    # tile the angle too\n",
    "    angles_tiled = torch.tensor(repeat(angles, 'n c -> (n b) c', b = num_bins))\n",
    "    return pos, angles_tiled, origins_tiled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUpSamples(origins, angles, gt_distance, num_rolls = 1):\n",
    "    upsample_pos = torch.empty(0,3)\n",
    "    upsample_ang = torch.empty(0,2)\n",
    "    upsample_gt_dist = torch.empty(0,1)\n",
    "\n",
    "    for num_roll in range(1, num_rolls+1):\n",
    "        # first, we prepare pairs of data, where one of them has a shorter ray, and another has a longer ray\n",
    "        gt_distance_rolled = torch.roll(gt_distance, num_roll, 0)\n",
    "        condition =  gt_distance < gt_distance_rolled\n",
    "        condition = rearrange(condition, 'a -> a 1')\n",
    "\n",
    "        dir = torch.tensor(sph2cart(angles))\n",
    "        gt_dist = rearrange(gt_distance, 'a -> a 1')\n",
    "        gt_distance_rolled = rearrange(gt_distance_rolled, 'a -> a 1')\n",
    "        pos = gt_dist * dir\n",
    "\n",
    "        pos_shorter = torch.where(condition, pos, torch.roll(pos, num_roll, 0))\n",
    "        origins_shorter = torch.where(condition, origins, torch.roll(origins, num_roll, 0))\n",
    "        angles_shorter = torch.where(condition, angles, torch.roll(angles, num_roll, 0))\n",
    "        gt_dist_shorter = torch.where(condition, gt_dist, gt_distance_rolled)\n",
    "\n",
    "        # pos_longer = torch.where(condition, torch.roll(pos, num_roll, 0), pos)\n",
    "        origins_longer = torch.where(condition, torch.roll(origins, num_roll, 0), origins)\n",
    "        angles_longer = torch.where(condition, torch.roll(angles, num_roll, 0), angles)\n",
    "        # gt_dist_longer = torch.where(condition, gt_distance_rolled, gt_dist)\n",
    "        \n",
    "        # check if angle between pairs are small\n",
    "        diff = torch.abs(angles_shorter - angles_longer)\n",
    "        mask_small_ang = (diff < 0.09).all(dim=1)   ### NOTE: hardcoded 0.09 radient difference max\n",
    "\n",
    "        # check if origin between pairs are small\n",
    "        diff2 = torch.abs(origins_shorter - origins_longer)\n",
    "        mask_small_org = (diff2 < 0.2).all(dim=1)   ### pass if coordinate in origin are less than 20cm in all dimensions\n",
    "        \n",
    "        # get masks for both cases\n",
    "        mask_same_org = mask_small_ang & mask_small_org\n",
    "\n",
    "        ### Handling case of same origin\n",
    "        # prepare set where rays are to be upsampled \n",
    "        # ensuring samples are at rays that are longer\n",
    "        angles_from_same_org = angles_longer[mask_same_org]\n",
    "        origins_from_same_org = origins_longer[mask_same_org]\n",
    "        gt_dist_to_same_org = gt_dist_shorter[mask_same_org] \n",
    "        pos_to_same_org = pos_shorter[mask_same_org]\n",
    "\n",
    "        if angles_from_same_org.shape[0] == 0:\n",
    "            continue   # skip if there are no points available for upsampling\n",
    "\n",
    "        # calculate upsampling position\n",
    "        num_bins = 20 \n",
    "        t = torch.linspace(0,1, num_bins).expand(angles_from_same_org.shape[0], num_bins)  # [batch_size, num_bins]\n",
    "        \n",
    "        # preterb the spacing\n",
    "        mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "        lower = torch.cat((t[:, :1], mid), -1)\n",
    "        upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "        u = torch.rand(t.shape)\n",
    "        t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "        t = rearrange(t, 'a b -> b a')  # [num_bins, batch_size]  take transpose so that multiplication can broadcast\n",
    "        t = rearrange(t, 'a b -> (b a) 1')\n",
    "        t = torch.sqrt(t)\n",
    "        \n",
    "        # get the sampling positions\n",
    "        origins_from_tiled = repeat(origins_from_same_org, 'n c -> (n b) c', b = num_bins)\n",
    "        dir_from = torch.tensor(sph2cart(angles_from_same_org))\n",
    "        dir_from_tiled = repeat(dir_from, 'n c -> (n b) c', b = num_bins)\n",
    "        gt_dist_to_tiled = repeat(gt_dist_to_same_org, 'n c -> (n b) c', b = num_bins)     \n",
    "\n",
    "        pos_from = origins_from_tiled + dir_from_tiled * t * gt_dist_to_tiled\n",
    "        pos_to_tiled = repeat(pos_to_same_org, 'n c -> (n b) c', b = num_bins)\n",
    "\n",
    "        # also calculte the ground truth distance of our up sampled location\n",
    "        sample_sph = cart2sph(pos_from - pos_to_tiled)\n",
    "        sample_direction = torch.tensor(sample_sph[:,1:])\n",
    "        sample_gt_distance = torch.tensor(sample_sph[:,0])\n",
    "\n",
    "        # add one more dimension to sample_gt_distance\n",
    "        sample_gt_distance = rearrange(sample_gt_distance, 'a -> a 1')\n",
    "\n",
    "        upsample_pos = torch.vstack((upsample_pos, pos_from))\n",
    "        upsample_ang = torch.vstack((upsample_ang, sample_direction))\n",
    "        upsample_gt_dist = torch.vstack((upsample_gt_dist, sample_gt_distance))\n",
    "\n",
    "    # return pos_from , torch.tensor(sample_direction), torch.tensor(sample_gt_distance)\n",
    "    return upsample_pos, upsample_ang, upsample_gt_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTargetValues(sample_positions, gt_distance, origins):\n",
    "\n",
    "    # calculate distance from sample_position\n",
    "    temp = torch.tensor((sample_positions - origins)**2)\n",
    "    pos_distance = torch.sqrt(torch.sum(temp, dim=1, keepdim=True))\n",
    "\n",
    "    # find the \"projected\" value\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    values = sigmoid(-(pos_distance - gt_distance))\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.randn(10, 3)\n",
    "norms = points.norm(p=2, dim=1, keepdim=True)\n",
    "log_norms = torch.log(norms) / norms\n",
    "log_norms[norms <= 1] = 1  # Set log_norms to 1 for norms <= 1 to use as a multiplier without changing these points\n",
    "updated_points = points * (log_norms)\n",
    "resultant_tensor = updated_points\n",
    "resultant_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # constants\n",
    "# num_bins = 100\n",
    "# device = 'cuda'\n",
    "\n",
    "# # sample data for testing\n",
    "# dataset_path = r'datasets/testing1/'\n",
    "# points = loadData(dataset_path)\n",
    "# dataset = prepareData(points)\n",
    "# test_batch = torch.tensor(dataset[0:256,:])\n",
    "# gt_distance = test_batch[:,0]\n",
    "# angles = test_batch[:,1:3]\n",
    "# origins = test_batch[:,3:6]\n",
    "# upsample_pos, upsample_ang, upsample_gt_distance = getUpSamples(origins, angles,  gt_distance, num_rolls=5)\n",
    "# sample_pos, sample_ang, sample_org = getSamples(origins, angles, gt_distance, num_bins = num_bins)\n",
    "# gt_distance_tiled = repeat(gt_distance, 'b -> (b n) 1', n = num_bins)\n",
    "\n",
    "# # # stack the upsampled position to sampled positions\n",
    "# pos = (torch.vstack((sample_pos, upsample_pos))).to(device)\n",
    "# ang = (torch.vstack((sample_ang, upsample_ang))).to(device)\n",
    "# gt_dis = (torch.vstack((gt_distance_tiled,upsample_gt_distance))).to(device)\n",
    "# org = (torch.vstack((sample_org, upsample_pos))).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inference and training\n",
    "# model = LiDAR_NeRF(hidden_dim=512).to(device)\n",
    "# rendered_value = model(pos, ang)\n",
    "# sigmoid = nn.Sigmoid()\n",
    "# rendered_value_sigmoided = sigmoid(rendered_value)\n",
    "# actual_value_sigmoided = (getTargetValues(pos, gt_dis, org)).to(dtype = torch.float32)\n",
    "# # loss = lossBCE(rendered_value, actual_value_sigmoided)  # + lossEikonal(model)\n",
    "# loss_bce = nn.BCELoss()\n",
    "# loss = loss_bce(rendered_value_sigmoided, actual_value_sigmoided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, dataloader, device = 'cuda', epoch = int(1e5), num_bins = 100):\n",
    "    training_losses = []\n",
    "    for _ in tqdm(range(epoch)):\n",
    "        for batch in dataloader:\n",
    "            # parse the batch\n",
    "            ground_truth_distance = batch[:,0]\n",
    "            angles = batch[:,1:3]\n",
    "            origins = batch[:,3:6]\n",
    "            upsample_pos, upsample_ang, upsample_gt_distance = getUpSamples(origins, angles, ground_truth_distance, num_rolls=100)\n",
    "            sample_pos, sample_ang, sample_org = getSamples(origins, angles, ground_truth_distance, num_bins=num_bins)\n",
    "            # tile distances\n",
    "            gt_distance_tiled = repeat(ground_truth_distance, 'b -> (b n) 1', n=num_bins)\n",
    "\n",
    "            # stack the upsampled position to sampled positions\n",
    "            pos = (torch.vstack((sample_pos, upsample_pos))).to(device)\n",
    "            ang = (torch.vstack((sample_ang, upsample_ang))).to(device)\n",
    "            gt_dis = (torch.vstack((gt_distance_tiled,upsample_gt_distance))).to(device)\n",
    "            org = (torch.vstack((sample_org, upsample_pos))).to(device)\n",
    "            \n",
    "            # inference\n",
    "            rendered_value = model(pos, ang)\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            rendered_value_sigmoid = sigmoid(rendered_value)\n",
    "            actual_value_sigmoided = (getTargetValues(pos, gt_dis, org)).to(dtype = torch.float32)\n",
    "\n",
    "            # loss = lossBCE(rendered_value, actual_value_sigmoided)  # + lossEikonal(model)\n",
    "            # back propergate\n",
    "            loss_bce = nn.BCELoss()\n",
    "            loss = loss_bce(rendered_value_sigmoid, actual_value_sigmoided)\n",
    "\n",
    "            # loss_mse = nn.MSELoss()\n",
    "            # loss = loss_mse(rendered_value, actual_value_sigmoided)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_losses.append(loss.item())\n",
    "        scheduler.step()\n",
    "        print(loss.item())\n",
    "    return training_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "loaded data\n",
      "prepared data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]/tmp/ipykernel_12558/2544353153.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dir_x = torch.tensor(np.cos(elev)*np.cos(pan))      # [batch_size]\n",
      "/tmp/ipykernel_12558/2544353153.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dir_y = torch.tensor(np.cos(elev)*np.sin(pan))      # [batch_size]\n",
      "/tmp/ipykernel_12558/2544353153.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dir_z = torch.tensor(np.sin(elev))\n",
      "/tmp/ipykernel_12558/2544353153.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gt_tensor = torch.tensor(ground_truth_distance)\n",
      "/tmp/ipykernel_12558/2544353153.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos = torch.tensor(origins_tiled) + multiplied\n",
      "/tmp/ipykernel_12558/2544353153.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  angles_tiled = torch.tensor(repeat(angles, 'n c -> (n b) c', b = num_bins))\n",
      "  6%|▋         | 1/16 [07:38<1:54:42, 458.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3538864254951477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 2/16 [15:16<1:46:51, 457.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34791305661201477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3/16 [22:56<1:39:28, 459.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33681416511535645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 4/16 [30:33<1:31:39, 458.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33844879269599915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 5/16 [38:09<1:23:50, 457.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34546980261802673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 6/16 [45:39<1:15:47, 454.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34417593479156494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 7/16 [53:07<1:07:55, 452.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3485005795955658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 8/16 [1:00:38<1:00:18, 452.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34286051988601685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 9/16 [1:08:25<53:17, 456.81s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3365975618362427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 10/16 [1:16:03<45:43, 457.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3271607756614685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 11/16 [1:23:34<37:55, 455.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3373619318008423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 12/16 [1:31:03<30:13, 453.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.328958123922348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 13/16 [1:38:30<22:34, 451.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3309963047504425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 14/16 [1:45:59<15:01, 450.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32705679535865784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 15/16 [1:53:33<07:31, 451.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3341082036495209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [2:01:02<00:00, 453.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32408761978149414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")\n",
    "# dataset_path = r'datasets/testing1/'\n",
    "# points = loadData(dataset_path)\n",
    "print(\"loaded data\")\n",
    "# data_matrix = prepareData(points)\n",
    "with open('datasets/manual_register.npy', 'rb') as f:\n",
    "    data_matrix = np.load(f)\n",
    "data_matrix = np.load()\n",
    "print(\"prepared data\")\n",
    "training_dataset = torch.from_numpy(data_matrix)\n",
    "data_loader = DataLoader(training_dataset, batch_size=512, shuffle = True)\n",
    "model = LiDAR_NeRF(hidden_dim=512, embedding_dim_dir=15, device = device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2, 4, 8, 16], gamma=0.5)\n",
    "losses = train(model, optimizer, scheduler, data_loader, epoch = 16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model\n",
    "torch.save(model.state_dict(), 'local/models/version3_trial1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load the model and try to \"visualize\" the model's datapoints\n",
    "model_evel = LiDAR_NeRF(hidden_dim=512, embedding_dim_dir=15, device = 'cpu')\n",
    "model_evel.load_state_dict(torch.load('local/models/version3_trial1.pth'))\n",
    "model_evel.eval(); # Set the model to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Render some structured pointcloud for evaluation\n",
    "with torch.no_grad():\n",
    "    dist = 0.1 # initial distanc forvisualization\n",
    "    pos = torch.zeros((100000,3))\n",
    "    ele = torch.linspace(-0.34, 0.3, 100)\n",
    "    pan = torch.linspace(-3.14, 3.14, 1000)\n",
    "    ele_tiled = repeat(ele, 'n -> (r n) 1', r = 1000)\n",
    "    pan_tiled = repeat(pan, 'n -> (n r) 1', r = 100)\n",
    "    ang = torch.cat((ele_tiled, pan_tiled), dim=1)\n",
    "\n",
    "    # direction for each \"point\" from camera centre\n",
    "    directions = torch.tensor(sph2cart(np.array(ang)))\n",
    "\n",
    "    for i in range(500):\n",
    "        output2 = model_evel(pos, ang)\n",
    "        temp = torch.sign(output2)\n",
    "        pos += directions * dist * temp\n",
    "        # dist /= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Render some structured pointcloud for evaluation\n",
    "with torch.no_grad():\n",
    "    dist = 32 # initial distanc forvisualization\n",
    "    pos = torch.zeros((100000,3))\n",
    "    pos[:,1] += 2\n",
    "    ele = torch.linspace(-0.34, 0.3, 100)\n",
    "    pan = torch.linspace(-3.14, 3.14, 1000)\n",
    "    ele_tiled = repeat(ele, 'n -> (r n) 1', r = 1000)\n",
    "    pan_tiled = repeat(pan, 'n -> (n r) 1', r = 100)\n",
    "    ang = torch.cat((ele_tiled, pan_tiled), dim=1)\n",
    "\n",
    "    # direction for each \"point\" from camera centre\n",
    "    directions = torch.tensor(sph2cart(np.array(ang)))\n",
    "\n",
    "    for i in range(10):\n",
    "        output2 = model_evel(pos, ang)\n",
    "        temp = torch.sign(output2)\n",
    "        pos += directions * dist * temp\n",
    "        dist /= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'datasets/testing1/'\n",
    "a = loadData(path)\n",
    "data = prepareData(a)\n",
    "r = data[:,0]\n",
    "r = rearrange(r, 'a -> a 1')\n",
    "ang = data[:,1:3]\n",
    "o = data[:,3:6]\n",
    "dir = sph2cart(ang)\n",
    "pos_relative = dir*r\n",
    "pos_np = pos_relative-o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save to csv for visualization\n",
    "df_temp = pd.read_csv('local/visualize/dummy.csv')\n",
    "# pos_np = pos.numpy()\n",
    "df_temp = df_temp.head(pos_np.shape[0])\n",
    "df_temp['X'] = pos_np[:,0]\n",
    "df_temp['Y'] = pos_np[:,1]\n",
    "df_temp['Z'] = pos_np[:,2]\n",
    "df_temp.to_csv('local/visualize/register_check1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidar_nerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
