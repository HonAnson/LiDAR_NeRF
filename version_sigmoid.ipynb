{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "from einops import rearrange, repeat\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperation (NOTE: Using meter as unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "# convert pointcloud from cartisean coordinate to spherical coordinate\n",
    "def cart2sph(xyz):\n",
    "    x = xyz[:,0]\n",
    "    y = xyz[:,1]\n",
    "    z = xyz[:,2]\n",
    "    XsqPlusYsq = x**2 + y**2\n",
    "    r = np.sqrt(list(XsqPlusYsq + z**2))\n",
    "    elev = np.arctan2(list(z), np.sqrt(list(XsqPlusYsq)))\n",
    "    pan = np.arctan2(list(x), list(y))\n",
    "\n",
    "    output = np.array([r, elev, pan])\n",
    "    return rearrange(output, 'a b -> b a') #take transpose\n",
    "\n",
    "\n",
    "def sph2cart(ang):\n",
    "    ele = ang[:,0]\n",
    "    pan = ang[:,1]\n",
    "    x = np.cos(ele)*np.cos(pan)\n",
    "    y = np.cos(ele)*np.sin(pan)\n",
    "    z = np.sin(ele)\n",
    "    output = np.array([x,y,z])\n",
    "    return rearrange(output, 'a b -> b a') #take transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    # Specify the directory path\n",
    "    dataset_path = 'datasets/testing1'\n",
    "\n",
    "    # List all files in the specified path, ignoring directories\n",
    "    files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "    files.sort()\n",
    "\n",
    "    # read the files\n",
    "    points_xyz = []\n",
    "    for s in files:\n",
    "        path = 'datasets/testing1/' + s\n",
    "        df = pd.read_csv(path)\n",
    "        a = df.to_numpy()\n",
    "        points_xyz.append(a[:,8:11])\n",
    "    return points_xyz\n",
    "\n",
    "def prepareData(points_xyz):\n",
    "    # Find the fiew direction of each points:\n",
    "    # NOTE: points in spherical coordinate are arranged: [r, elev, pan]\n",
    "    points_sphere = []\n",
    "    for points in points_xyz:\n",
    "        points_sphere.append(cart2sph(points))\n",
    "\n",
    "    ### Process the data\n",
    "    # Translation vectors for points in each view, we are using camera centre at first frame as origin of world coordinate\n",
    "    # NOTE: translation vectors below are found by assuming transformation between frames are translations, and obatined by manually finding corrspondance\n",
    "    # They are translation of the same corrspondance across different frames\n",
    "    # HARD CODED HERE\n",
    "    t0 = np.array([0,0,0])\n",
    "    t1 = np.array([-0.671,-0.016,0.215])\n",
    "    t2 = np.array([-1.825,-0.091,0.147])\n",
    "    t3 = np.array([-2.661,-0.263,0.166])\n",
    "    t4 = np.array([-3.607,-0.156,0.039])\n",
    "    translations = [t0, t1, t2, t3, t4]\n",
    "\n",
    "    # camera centre locations\n",
    "    centres = [-t for t in translations]\n",
    "    centres_data = []\n",
    "    for i,c in enumerate(centres):\n",
    "        l = len(points_sphere[i])\n",
    "        temp = np.tile(c, (l, 1))\n",
    "        centres_data.append(temp)\n",
    "\n",
    "    # stack the points into one big matrix\n",
    "    stacked = []\n",
    "    for i in range(len(points_sphere)):\n",
    "        temp = np.hstack((points_sphere[i], centres_data[i]))\n",
    "        stacked.append(temp)\n",
    "\n",
    "    dataset = np.array([])\n",
    "    for i in range(len(stacked)):\n",
    "        if i == 0:\n",
    "            dataset = stacked[i]\n",
    "        else:\n",
    "            dataset = np.vstack((dataset, stacked[i]))\n",
    "    np.random.shuffle(dataset)\n",
    "\n",
    "    # Mid pass filter, for distance value between 2 and 50 meter\n",
    "    mask1 = dataset[:,0] > 2\n",
    "    dataset = dataset[mask1]\n",
    "    mask2 = dataset[:,0] < 50\n",
    "    dataset = dataset[mask2]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiDAR_NeRF(nn.Module):\n",
    "    def __init__(self, embedding_dim_pos = 10, embedding_dim_dir = 4, hidden_dim = 256, device = 'cuda'):\n",
    "        super(LiDAR_NeRF, self).__init__()\n",
    "        self.device = device\n",
    "        self.embedding_dim_dir = embedding_dim_dir\n",
    "        self.embedding_dim_pos = embedding_dim_pos\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim_pos * 6 + 3 + embedding_dim_dir * 4 + 2, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim_pos * 6 + 3 + embedding_dim_dir * 4 + 2 + hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def positional_encoding(x, L):\n",
    "        out = [x]\n",
    "        for j in range(L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    def forward(self, o, d):\n",
    "        emb_x = self.positional_encoding(o, self.embedding_dim_pos)\n",
    "        emb_d = self.positional_encoding(d, self.embedding_dim_dir)\n",
    "        input = torch.hstack((emb_x,emb_d)).to(dtype=torch.float32)\n",
    "        temp = self.block1(input)\n",
    "        input2 = torch.hstack((temp, input)).to(dtype=torch.float32) # add skip input\n",
    "        output = self.block2(input2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_positions(origins, angles, ground_truth_distance, num_bins = 100, device = 'cpu'):\n",
    "    elev = angles[:,0]\n",
    "    pan = angles[:,1]\n",
    "    dir_x = torch.tensor(np.cos(elev)*np.cos(pan))      # [batch_size]\n",
    "    dir_y = torch.tensor(np.cos(elev)*np.sin(pan))      # [batch_size]\n",
    "    dir_z = torch.tensor(np.sin(elev))\n",
    "    gt_tensor = torch.tensor(ground_truth_distance)\n",
    "    # create a list of magnitudes with even spacing from 0 to 1\n",
    "    t = torch.linspace(0,1, num_bins, device=device).expand(dir_x.shape[0], num_bins)  # [batch_size, num_bins]\n",
    "    \n",
    "    # preterb the spacing\n",
    "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "    lower = torch.cat((t[:, :1], mid), -1)\n",
    "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape, device = device)\n",
    "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "    t = rearrange(t, 'a b -> b a')  # [num_bins, batch_size]  take transpose so that multiplication can broadcast\n",
    "\n",
    "    # multiply the magnitude to ground truth distance and add 3 meter\n",
    "    t = gt_tensor*t\n",
    "    t += 3\n",
    "\n",
    "    # convert magnitudes into positions by multiplying it to the unit vector\n",
    "    pos_x = dir_x*t     # [num_bins, batch_size]\n",
    "    pos_y = dir_y*t\n",
    "    pos_z = dir_z*t\n",
    "    # concat them for output\n",
    "    multiplied = rearrange([pos_x,pos_y,pos_z], 'c b n  -> (n b) c')   # [num_bin*batchsize, 3]\n",
    "    # tile the origin values\n",
    "    origins_tiled = repeat(origins, 'n c -> (n b) c', b = num_bins) # [num_bin*batch_size, 3]\n",
    "    pos = torch.tensor(origins_tiled) + multiplied\n",
    "    # tile the angle too\n",
    "    angles_tiled = torch.tensor(repeat(angles, 'n c -> (n b) c', b = num_bins))\n",
    "    return pos, angles_tiled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns pytorch tensor of sigmoid of projected SDF\n",
    "def get_actual_value(sample_positions, gt_distance, num_bins=100):\n",
    "    # tile distances\n",
    "    gt_distance_tiled = repeat(gt_distance, 'b -> (b n) 1', n=num_bins)\n",
    "    # calculate distance from sample_position\n",
    "    temp = torch.tensor(sample_positions**2)\n",
    "    pos_distance = torch.sqrt(torch.sum(temp, dim=1, keepdim=True))\n",
    "    # find the \"projected\" value\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    values = sigmoid(-(pos_distance - gt_distance_tiled))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9493/25219483.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  temp = torch.tensor(sample_positions**2)\n"
     ]
    }
   ],
   "source": [
    "# sample data for testing\n",
    "points = loadData()\n",
    "dataset = prepareData(points)\n",
    "test_batch = dataset[128:256,:]\n",
    "ground_truth_distance = test_batch[:,0]\n",
    "angles = test_batch[:,1:3]\n",
    "origin = test_batch[:,3:7]\n",
    "pos, ang = get_sample_positions(origin, angles, ground_truth_distance ,num_bins=100)\n",
    "val = (get_actual_value(pos, ground_truth_distance)).to(dtype = torch.float32)\n",
    "\n",
    "model = LiDAR_NeRF(hidden_dim=256)\n",
    "rendered = model(pos, ang)\n",
    "sigmoid = nn.Sigmoid()\n",
    "rendered_sigmoid = sigmoid(rendered)\n",
    "temp = torch.zeros_like(pos)\n",
    "\n",
    "# for x in val:\n",
    "#     print(x)\n",
    "# print(min(rendered))\n",
    "# loss_bce = nn.BCELoss()\n",
    "# loss = loss_bce(rendered_sigmoid, val)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, dataloader, device = 'cpu', epoch = int(1e5), num_bins = 100):\n",
    "    training_losses = []\n",
    "    for _ in tqdm(range(epoch)):\n",
    "        for batch in dataloader:\n",
    "            # parse the batch\n",
    "            ground_truth_distance = batch[:,0]\n",
    "            angles = batch[:,1:3]\n",
    "            origin = batch[:,3:7]\n",
    "            \n",
    "            sample_positions, sample_angles = get_sample_positions(origin, angles, ground_truth_distance, num_bins=num_bins)\n",
    "            print(sample_positions[0:10])\n",
    "            \n",
    "            rendered_value = model(sample_positions.to(device), sample_angles.to(device))\n",
    "            \n",
    "            sigmoid = nn.Sigmoid()\n",
    "            rendered_value_sigmoid = sigmoid(rendered_value)\n",
    "            actual_value_sigmoided = (get_actual_value(sample_positions.to(device), ground_truth_distance.to(device))).to(dtype = torch.float32)\n",
    "            # print(rendered_value_sigmoid[0:10]) \n",
    "            # loss = lossBCE(rendered_value, actual_value_sigmoided)  # + lossEikonal(model)\n",
    "            loss_bce = nn.BCELoss()\n",
    "            loss = loss_bce(rendered_value_sigmoid, actual_value_sigmoided)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_losses.append(loss.item())\n",
    "        scheduler.step()\n",
    "        print(loss.item())\n",
    "    return training_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2792, -0.4808, -0.0902],\n",
      "        [-1.8001, -0.5768, -0.0807],\n",
      "        [-2.0652, -0.6256, -0.0758],\n",
      "        [-2.5410, -0.7132, -0.0671],\n",
      "        [-3.0265, -0.8027, -0.0583],\n",
      "        [-3.2955, -0.8522, -0.0533],\n",
      "        [-4.1795, -1.0150, -0.0372],\n",
      "        [-4.3110, -1.0393, -0.0348],\n",
      "        [-4.8093, -1.1311, -0.0257],\n",
      "        [-5.2568, -1.2135, -0.0175]], dtype=torch.float64)\n",
      "tensor([[-0.4046, -1.9273, -0.3664],\n",
      "        [-0.4720, -1.9883, -0.3730],\n",
      "        [-0.5996, -2.1038, -0.3855],\n",
      "        [-0.6906, -2.1862, -0.3945],\n",
      "        [-0.8156, -2.2993, -0.4068],\n",
      "        [-0.9497, -2.4208, -0.4200],\n",
      "        [-1.0933, -2.5507, -0.4341],\n",
      "        [-1.1557, -2.6072, -0.4402],\n",
      "        [-1.2635, -2.7048, -0.4509],\n",
      "        [-1.4008, -2.8290, -0.4644]], dtype=torch.float64)\n",
      "tensor([[ 2.7216, -2.7609,  0.2752],\n",
      "        [ 2.7559, -2.8701,  0.2914],\n",
      "        [ 2.8187, -3.0699,  0.3210],\n",
      "        [ 2.8575, -3.1931,  0.3392],\n",
      "        [ 2.8863, -3.2847,  0.3527],\n",
      "        [ 2.9773, -3.5744,  0.3956],\n",
      "        [ 3.0202, -3.7106,  0.4158],\n",
      "        [ 3.0411, -3.7772,  0.4257],\n",
      "        [ 3.1046, -3.9792,  0.4556],\n",
      "        [ 3.1393, -4.0896,  0.4719]], dtype=torch.float64)\n",
      "tensor([[-0.7737, -1.2365,  0.6223],\n",
      "        [-1.2451, -1.4773,  0.7618],\n",
      "        [-1.5676, -1.6421,  0.8573],\n",
      "        [-1.7774, -1.7493,  0.9194],\n",
      "        [-2.3114, -2.0221,  1.0775],\n",
      "        [-2.5732, -2.1558,  1.1550],\n",
      "        [-2.8870, -2.3161,  1.2479],\n",
      "        [-3.1098, -2.4299,  1.3138],\n",
      "        [-3.5002, -2.6293,  1.4294],\n",
      "        [-3.7763, -2.7703,  1.5111]], dtype=torch.float64)\n",
      "tensor([[-1.8273,  2.3182, -0.5660],\n",
      "        [-1.8528,  2.3506, -0.5739],\n",
      "        [-1.8875,  2.3946, -0.5847],\n",
      "        [-1.8985,  2.4086, -0.5881],\n",
      "        [-1.9473,  2.4705, -0.6032],\n",
      "        [-1.9779,  2.5093, -0.6127],\n",
      "        [-1.9790,  2.5107, -0.6130],\n",
      "        [-2.0210,  2.5640, -0.6261],\n",
      "        [-2.0415,  2.5900, -0.6324],\n",
      "        [-2.0874,  2.6483, -0.6466]], dtype=torch.float64)\n",
      "tensor([[ 2.2452,  2.5888, -0.3996],\n",
      "        [ 2.3418,  2.7468, -0.4109],\n",
      "        [ 2.3977,  2.8381, -0.4174],\n",
      "        [ 2.5002,  3.0057, -0.4295],\n",
      "        [ 2.5414,  3.0730, -0.4343],\n",
      "        [ 2.5975,  3.1647, -0.4409],\n",
      "        [ 2.7170,  3.3600, -0.4549],\n",
      "        [ 2.7719,  3.4498, -0.4613],\n",
      "        [ 2.8585,  3.5913, -0.4715],\n",
      "        [ 2.9432,  3.7298, -0.4814]], dtype=torch.float64)\n",
      "tensor([[ 1.0256, -2.8617, -0.1482],\n",
      "        [ 0.9945, -2.9767, -0.1483],\n",
      "        [ 0.9406, -3.1760, -0.1483],\n",
      "        [ 0.8836, -3.3863, -0.1484],\n",
      "        [ 0.7747, -3.7886, -0.1486],\n",
      "        [ 0.7335, -3.9407, -0.1487],\n",
      "        [ 0.6701, -4.1750, -0.1488],\n",
      "        [ 0.6322, -4.3151, -0.1488],\n",
      "        [ 0.5354, -4.6727, -0.1490],\n",
      "        [ 0.5028, -4.7932, -0.1490]], dtype=torch.float64)\n",
      "tensor([[ 1.5466,  2.5892, -0.1855],\n",
      "        [ 1.6261,  2.7223, -0.1950],\n",
      "        [ 1.6756,  2.8051, -0.2010],\n",
      "        [ 1.7362,  2.9066, -0.2082],\n",
      "        [ 1.8747,  3.1384, -0.2249],\n",
      "        [ 1.9283,  3.2282, -0.2313],\n",
      "        [ 2.0058,  3.3579, -0.2406],\n",
      "        [ 2.1087,  3.5302, -0.2529],\n",
      "        [ 2.1649,  3.6243, -0.2597],\n",
      "        [ 2.2491,  3.7653, -0.2698]], dtype=torch.float64)\n",
      "tensor([[ 3.5198,  2.5735, -0.3236],\n",
      "        [ 3.5762,  2.6562, -0.3295],\n",
      "        [ 3.7012,  2.8393, -0.3425],\n",
      "        [ 3.8000,  2.9839, -0.3528],\n",
      "        [ 3.8703,  3.0868, -0.3601],\n",
      "        [ 3.9461,  3.1979, -0.3680],\n",
      "        [ 4.0284,  3.3184, -0.3766],\n",
      "        [ 4.1134,  3.4430, -0.3855],\n",
      "        [ 4.1610,  3.5127, -0.3904],\n",
      "        [ 4.2490,  3.6416, -0.3996]], dtype=torch.float64)\n",
      "tensor([[-1.2016, -0.0265, -0.5222],\n",
      "        [-1.2639, -0.0289, -0.5299],\n",
      "        [-1.5027, -0.0382, -0.5595],\n",
      "        [-1.6078, -0.0422, -0.5726],\n",
      "        [-1.7850, -0.0491, -0.5945],\n",
      "        [-1.8926, -0.0533, -0.6079],\n",
      "        [-2.0703, -0.0602, -0.6299],\n",
      "        [-2.1316, -0.0626, -0.6375],\n",
      "        [-2.4009, -0.0730, -0.6709],\n",
      "        [-2.5172, -0.0775, -0.6853]], dtype=torch.float64)\n",
      "tensor([[-1.9776, -1.3539, -0.7878],\n",
      "        [-2.0344, -1.3833, -0.8001],\n",
      "        [-2.1246, -1.4300, -0.8196],\n",
      "        [-2.2343, -1.4867, -0.8433],\n",
      "        [-2.2790, -1.5098, -0.8530],\n",
      "        [-2.3647, -1.5541, -0.8715],\n",
      "        [-2.5035, -1.6259, -0.9015],\n",
      "        [-2.5897, -1.6705, -0.9202],\n",
      "        [-2.6628, -1.7083, -0.9360],\n",
      "        [-2.7143, -1.7349, -0.9471]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m)\n\u001b[1;32m     11\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mMultiStepLR(optimizer, milestones\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m16\u001b[39m], gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, dataloader, device, epoch, num_bins)\u001b[0m\n\u001b[1;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 25\u001b[0m     training_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")\n",
    "points = loadData()\n",
    "print(\"loaded data\")\n",
    "data_matrix = prepareData(points)\n",
    "print(\"prepared data\")\n",
    "training_dataset = torch.from_numpy(data_matrix)\n",
    "data_loader = DataLoader(training_dataset, batch_size=1024, shuffle = True)\n",
    "model = LiDAR_NeRF(hidden_dim=512, embedding_dim_dir=10, device = device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2, 4, 8, 16], gamma=0.5)\n",
    "losses = train(model, optimizer, scheduler, data_loader, epoch = 4, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model\n",
    "torch.save(model.state_dict(), 'local/models/version1_trial3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load the model and try to \"visualize\" the model's datapoints\n",
    "model_evel = LiDAR_NeRF(hidden_dim=512, embedding_dim_dir=10, device = 'cpu')\n",
    "model_evel.load_state_dict(torch.load('local/models/version1_trial3.pth'))\n",
    "model_evel.eval(); # Set the model to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23911/25219483.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  temp = torch.tensor(sample_positions**2)\n",
      "/tmp/ipykernel_23911/3312487041.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_tensor = torch.tensor(pos)\n",
      "/tmp/ipykernel_23911/3312487041.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ang_tensor = torch.tensor(ang)\n"
     ]
    }
   ],
   "source": [
    "# sample data for testing\n",
    "points = loadData()\n",
    "dataset = prepareData(points)\n",
    "test_batch = dataset[0:512,:]\n",
    "ground_truth_distance = test_batch[:,0]\n",
    "angles = test_batch[:,1:3]\n",
    "origin = test_batch[:,3:]\n",
    "pos, ang = get_sample_positions(origin, angles, ground_truth_distance, num_bins=100)\n",
    "# pos = torch.zeros_like(pos)\n",
    "val = (get_actual_value(pos, ground_truth_distance)).to(dtype = torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pos_tensor = torch.tensor(pos)\n",
    "    ang_tensor = torch.tensor(ang)\n",
    "    output = model_evel(pos_tensor, ang_tensor)\n",
    "\n",
    "sig = nn.Sigmoid()\n",
    "output_sigmoided = sig(output)\n",
    "lossBCE = nn.BCELoss()\n",
    "loss = lossBCE(val, output_sigmoided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0745],\n",
       "        [0.0711],\n",
       "        [0.0677],\n",
       "        [0.0668],\n",
       "        [0.0698],\n",
       "        [0.0666],\n",
       "        [0.0620],\n",
       "        [0.0588],\n",
       "        [0.0581],\n",
       "        [0.0562],\n",
       "        [0.0542],\n",
       "        [0.0527],\n",
       "        [0.0524],\n",
       "        [0.0468],\n",
       "        [0.0471],\n",
       "        [0.0464],\n",
       "        [0.0428],\n",
       "        [0.0416],\n",
       "        [0.0389],\n",
       "        [0.0401],\n",
       "        [0.0376],\n",
       "        [0.0341],\n",
       "        [0.0328],\n",
       "        [0.0316],\n",
       "        [0.0331],\n",
       "        [0.0330],\n",
       "        [0.0320],\n",
       "        [0.0291],\n",
       "        [0.0288],\n",
       "        [0.0242],\n",
       "        [0.0245],\n",
       "        [0.0242],\n",
       "        [0.0219],\n",
       "        [0.0214],\n",
       "        [0.0207],\n",
       "        [0.0201],\n",
       "        [0.0196],\n",
       "        [0.0195],\n",
       "        [0.0177],\n",
       "        [0.0174],\n",
       "        [0.0163],\n",
       "        [0.0153],\n",
       "        [0.0146],\n",
       "        [0.0136],\n",
       "        [0.0138],\n",
       "        [0.0147],\n",
       "        [0.0137],\n",
       "        [0.0129],\n",
       "        [0.0117],\n",
       "        [0.0108],\n",
       "        [0.0119],\n",
       "        [0.0104],\n",
       "        [0.0104],\n",
       "        [0.0103],\n",
       "        [0.0094],\n",
       "        [0.0088],\n",
       "        [0.0084],\n",
       "        [0.0085],\n",
       "        [0.0086],\n",
       "        [0.0078],\n",
       "        [0.0074],\n",
       "        [0.0070],\n",
       "        [0.0068],\n",
       "        [0.0066],\n",
       "        [0.0064],\n",
       "        [0.0061],\n",
       "        [0.0059],\n",
       "        [0.0054],\n",
       "        [0.0052],\n",
       "        [0.0046],\n",
       "        [0.0048],\n",
       "        [0.0046],\n",
       "        [0.0040],\n",
       "        [0.0040],\n",
       "        [0.0038],\n",
       "        [0.0038],\n",
       "        [0.0034],\n",
       "        [0.0034],\n",
       "        [0.0032],\n",
       "        [0.0032],\n",
       "        [0.0030],\n",
       "        [0.0026],\n",
       "        [0.0025],\n",
       "        [0.0027],\n",
       "        [0.0024],\n",
       "        [0.0022],\n",
       "        [0.0022],\n",
       "        [0.0019],\n",
       "        [0.0021],\n",
       "        [0.0017],\n",
       "        [0.0017],\n",
       "        [0.0016],\n",
       "        [0.0015],\n",
       "        [0.0015],\n",
       "        [0.0014],\n",
       "        [0.0013],\n",
       "        [0.0012],\n",
       "        [0.0012],\n",
       "        [0.0011],\n",
       "        [0.0011]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sigmoided[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0759])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig = nn.Sigmoid()\n",
    "a = torch.tensor([-2.5], dtype = torch.float)\n",
    "sig(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Render some structured pointcloud for evaluation\n",
    "with torch.no_grad():\n",
    "    dist = 1 # initial distanc forvisualization\n",
    "    pos = torch.zeros((100000,3))\n",
    "    ele = torch.linspace(-0.34, 0.3, 100)\n",
    "    pan = torch.linspace(-3.14, 3.14, 1000)\n",
    "    ele_tiled = repeat(ele, 'n -> (r n) 1', r = 1000)\n",
    "    pan_tiled = repeat(pan, 'n -> (n r) 1', r = 100)\n",
    "    ang = torch.cat((ele_tiled, pan_tiled), dim=1)\n",
    "\n",
    "    # direction for each \"point\" from camera centre\n",
    "    directions = torch.tensor(sph2cart(np.array(ang)))\n",
    "\n",
    "    for i in range(50):\n",
    "        output2 = model_evel(pos, ang)\n",
    "        temp = torch.sign(output2)\n",
    "        pos += directions * dist * temp\n",
    "        # dist /= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Version  Slot ID  LiDAR Index  Rsvd  Error Code  Timestamp Type  Data Type  \\\n",
      "0        5        7            1     0  0x00000200               0          0   \n",
      "1        5        7            1     0  0x00000200               0          0   \n",
      "2        5        7            1     0  0x00000200               0          0   \n",
      "3        5        7            1     0  0x00000200               0          0   \n",
      "4        5        7            1     0  0x00000200               0          0   \n",
      "\n",
      "      Timestamp          X         Y         Z  Reflectivity  Tag  Ori_x  \\\n",
      "0  339330000000 -11.313043 -0.018017 -4.001845            24    0   8128   \n",
      "1  339330000000 -11.338676 -0.018057 -3.928627            24    0   8132   \n",
      "2  339330000000 -11.363835 -0.018097 -3.855245            24    0   8137   \n",
      "3  339330000000 -11.388520 -0.018137 -3.781701            28    0   8132   \n",
      "4  339330000000 -11.412731 -0.018175 -3.708000            28    0   8132   \n",
      "\n",
      "   Ori_y  Ori_z  Ori_radius  Ori_theta  Ori_phi  \n",
      "0  23651   6826           0          0        0  \n",
      "1  23653   6788           0          0        0  \n",
      "2  23661   6751           0          0        0  \n",
      "3  23646   6707           0          0        0  \n",
      "4  23642   6665           0          0        0  \n"
     ]
    }
   ],
   "source": [
    "### Save to csv for visualization\n",
    "df_temp = pd.read_csv('local/visualize/dummy.csv')\n",
    "df_temp = df_temp.head(100000)\n",
    "pos_np = pos.numpy()\n",
    "df_temp['X'] = pos_np[:,0]\n",
    "df_temp['Y'] = pos_np[:,1]\n",
    "df_temp['Z'] = pos_np[:,2]\n",
    "print(df_temp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.to_csv('local/visualize/visualize.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidar_nerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
