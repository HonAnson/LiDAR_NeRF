{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "from einops import rearrange, repeat\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperation (NOTE: Using meter as unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "# convert pointcloud from cartisean coordinate to spherical coordinate\n",
    "def cart2sph(xyz):\n",
    "    x = xyz[:,0]\n",
    "    y = xyz[:,1]\n",
    "    z = xyz[:,2]\n",
    "    XsqPlusYsq = x**2 + y**2\n",
    "    r = np.sqrt(list(XsqPlusYsq + z**2))\n",
    "    elev = np.arctan2(list(z), np.sqrt(list(XsqPlusYsq)))\n",
    "    pan = np.arctan2(list(x), list(y))\n",
    "\n",
    "    output = np.array([r, elev, pan])\n",
    "    return rearrange(output, 'a b -> b a') #take transpose\n",
    "\n",
    "\n",
    "def sph2cart(ang):\n",
    "    ele = ang[:,0]\n",
    "    pan = ang[:,1]\n",
    "    x = np.cos(ele)*np.cos(pan)\n",
    "    y = np.cos(ele)*np.sin(pan)\n",
    "    z = np.sin(ele)\n",
    "    output = np.array([x,y,z])\n",
    "    return rearrange(output, 'a b -> b a') #take transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    # Specify the directory path\n",
    "    dataset_path = 'datasets/testing1'\n",
    "\n",
    "    # List all files in the specified path, ignoring directories\n",
    "    files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "    files.sort()\n",
    "\n",
    "    # read the files\n",
    "    points_xyz = []\n",
    "    for s in files:\n",
    "        path = 'datasets/testing1/' + s\n",
    "        df = pd.read_csv(path)\n",
    "        a = df.to_numpy()\n",
    "        points_xyz.append(a[:,8:11])\n",
    "    return points_xyz\n",
    "\n",
    "def prepareData(points_xyz):\n",
    "    # Find the fiew direction of each points:\n",
    "    # NOTE: points in spherical coordinate are arranged: [r, elev, pan]\n",
    "    points_sphere = []\n",
    "    for points in points_xyz:\n",
    "        points_sphere.append(cart2sph(points))\n",
    "\n",
    "    ### Process the data\n",
    "    # Translation vectors for points in each view, we are using camera centre at first frame as origin of world coordinate\n",
    "    # NOTE: translation vectors below are found by assuming transformation between frames are translations, and obatined by manually finding corrspondance\n",
    "    # They are translation of the same corrspondance across different frames\n",
    "    # HARD CODED HERE\n",
    "    t0 = np.array([0,0,0])\n",
    "    t1 = np.array([-0.671,-0.016,0.215])\n",
    "    t2 = np.array([-1.825,-0.091,0.147])\n",
    "    t3 = np.array([-2.661,-0.263,0.166])\n",
    "    t4 = np.array([-3.607,-0.156,0.039])\n",
    "    translations = [t0, t1, t2, t3, t4]\n",
    "\n",
    "    # camera centre locations\n",
    "    centres = [-t for t in translations]\n",
    "    centres_data = []\n",
    "    for i,c in enumerate(centres):\n",
    "        l = len(points_sphere[i])\n",
    "        temp = np.tile(c, (l, 1))\n",
    "        centres_data.append(temp)\n",
    "\n",
    "    # stack the points into one big matrix\n",
    "    stacked = []\n",
    "    for i in range(len(points_sphere)):\n",
    "        temp = np.hstack((points_sphere[i], centres_data[i]))\n",
    "        stacked.append(temp)\n",
    "\n",
    "    dataset = np.array([])\n",
    "    for i in range(len(stacked)):\n",
    "        if i == 0:\n",
    "            dataset = stacked[i]\n",
    "        else:\n",
    "            dataset = np.vstack((dataset, stacked[i]))\n",
    "    np.random.shuffle(dataset)\n",
    "\n",
    "    # Mid pass filter, for distance value between 2 and 50 meter\n",
    "    mask1 = dataset[:,0] > 2\n",
    "    dataset = dataset[mask1]\n",
    "    mask2 = dataset[:,0] < 50\n",
    "    dataset = dataset[mask2]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiDAR_NeRF(nn.Module):\n",
    "    def __init__(self, embedding_dim_pos = 10, embedding_dim_dir = 4, hidden_dim = 256, device = 'cuda'):\n",
    "        super(LiDAR_NeRF, self).__init__()\n",
    "        self.device = device\n",
    "        self.embedding_dim_dir = embedding_dim_dir\n",
    "        self.embedding_dim_pos = embedding_dim_pos\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim_pos * 6 + 3 + embedding_dim_dir * 4 + 2, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim_pos * 6 + 3 + embedding_dim_dir * 4 + 2 + hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def positional_encoding(x, L):\n",
    "        out = [x]\n",
    "        for j in range(L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    def forward(self, o, d):\n",
    "        emb_x = self.positional_encoding(o, self.embedding_dim_pos)\n",
    "        emb_d = self.positional_encoding(d, self.embedding_dim_dir)\n",
    "        input = torch.hstack((emb_x,emb_d)).to(dtype=torch.float32)\n",
    "        temp = self.block1(input)\n",
    "        input2 = torch.hstack((temp, input)).to(dtype=torch.float32) # add skip input\n",
    "        output = self.block2(input2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossBCE(rendered_value, actual_value): \n",
    "    loss_bce = nn.CrossEntropyLoss()\n",
    "    loss = loss_bce(rendered_value, actual_value)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0101, 0.0202, 0.0303, 0.0404, 0.0505, 0.0606, 0.0707, 0.0808,\n",
       "         0.0909, 0.1010, 0.1111, 0.1212, 0.1313, 0.1414, 0.1515, 0.1616, 0.1717,\n",
       "         0.1818, 0.1919, 0.2020, 0.2121, 0.2222, 0.2323, 0.2424, 0.2525, 0.2626,\n",
       "         0.2727, 0.2828, 0.2929, 0.3030, 0.3131, 0.3232, 0.3333, 0.3434, 0.3535,\n",
       "         0.3636, 0.3737, 0.3838, 0.3939, 0.4040, 0.4141, 0.4242, 0.4343, 0.4444,\n",
       "         0.4545, 0.4646, 0.4747, 0.4848, 0.4949, 0.5051, 0.5152, 0.5253, 0.5354,\n",
       "         0.5455, 0.5556, 0.5657, 0.5758, 0.5859, 0.5960, 0.6061, 0.6162, 0.6263,\n",
       "         0.6364, 0.6465, 0.6566, 0.6667, 0.6768, 0.6869, 0.6970, 0.7071, 0.7172,\n",
       "         0.7273, 0.7374, 0.7475, 0.7576, 0.7677, 0.7778, 0.7879, 0.7980, 0.8081,\n",
       "         0.8182, 0.8283, 0.8384, 0.8485, 0.8586, 0.8687, 0.8788, 0.8889, 0.8990,\n",
       "         0.9091, 0.9192, 0.9293, 0.9394, 0.9495, 0.9596, 0.9697, 0.9798, 0.9899,\n",
       "         1.0000],\n",
       "        [0.0000, 0.0101, 0.0202, 0.0303, 0.0404, 0.0505, 0.0606, 0.0707, 0.0808,\n",
       "         0.0909, 0.1010, 0.1111, 0.1212, 0.1313, 0.1414, 0.1515, 0.1616, 0.1717,\n",
       "         0.1818, 0.1919, 0.2020, 0.2121, 0.2222, 0.2323, 0.2424, 0.2525, 0.2626,\n",
       "         0.2727, 0.2828, 0.2929, 0.3030, 0.3131, 0.3232, 0.3333, 0.3434, 0.3535,\n",
       "         0.3636, 0.3737, 0.3838, 0.3939, 0.4040, 0.4141, 0.4242, 0.4343, 0.4444,\n",
       "         0.4545, 0.4646, 0.4747, 0.4848, 0.4949, 0.5051, 0.5152, 0.5253, 0.5354,\n",
       "         0.5455, 0.5556, 0.5657, 0.5758, 0.5859, 0.5960, 0.6061, 0.6162, 0.6263,\n",
       "         0.6364, 0.6465, 0.6566, 0.6667, 0.6768, 0.6869, 0.6970, 0.7071, 0.7172,\n",
       "         0.7273, 0.7374, 0.7475, 0.7576, 0.7677, 0.7778, 0.7879, 0.7980, 0.8081,\n",
       "         0.8182, 0.8283, 0.8384, 0.8485, 0.8586, 0.8687, 0.8788, 0.8889, 0.8990,\n",
       "         0.9091, 0.9192, 0.9293, 0.9394, 0.9495, 0.9596, 0.9697, 0.9798, 0.9899,\n",
       "         1.0000]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.linspace(0,1,100).expand(2,100)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_positions(origins, angles, ground_truth_distance, num_bins = 100, device = 'cpu'):\n",
    "    elev = angles[:,0]\n",
    "    pan = angles[:,1]\n",
    "    dir_x = torch.tensor(np.cos(elev)*np.cos(pan))\n",
    "    dir_y = torch.tensor(np.cos(elev)*np.sin(pan))\n",
    "    dir_z = torch.tensor(np.sin(elev))\n",
    "\n",
    "    # create a list of magnitudes with even spacing\n",
    "    t = torch.linspace(0,1, num_bins, device=device).expand(dir_x.shape[0], num_bins)  # [batch_size, num_bins]\n",
    "    \n",
    "    # preterb the spacing\n",
    "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "    lower = torch.cat((t[:, :1], mid), -1)\n",
    "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape, device = device)\n",
    "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "    \n",
    "    # convert magnitudes into positions by multiplying unit vector in each direction\n",
    "    t = rearrange(t, 'a b -> b a')  # [num_bins, batch_size]\n",
    "    pos_x = dir_x*t     # [num_bins, batch_size]\n",
    "    pos_y = dir_y*t\n",
    "    pos_z = dir_z*t\n",
    "    multiplied = rearrange([pos_x,pos_y,pos_z], 'c b n  -> (n b) c')   # [num_bin*batchsize, 3]\n",
    "    # tile the origin values\n",
    "    origins_tiled = repeat(origins, 'n c -> (n b) c', b = num_bins) # [num_bin*batch_size, 3]\n",
    "    pos = torch.tensor(origins_tiled) + multiplied\n",
    "    # tile the angle for convenience\n",
    "    angles_tiled = torch.tensor(repeat(angles, 'n c -> (n b) c', b = num_bins))\n",
    "    return pos, angles_tiled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns pytorch tensor of sigmoid of projected SDF\n",
    "def get_actual_value(sample_positions, gt_distance, num_bins=100):\n",
    "    # tile distances\n",
    "    gt_distance_tiled = repeat(gt_distance, 'b -> (b n) 1', n=num_bins)\n",
    "    # calculate distance from sample_position\n",
    "    temp = torch.tensor(sample_positions**2)\n",
    "    pos_distance = torch.sqrt(torch.sum(temp, dim=1, keepdim=True))\n",
    "\n",
    "    # find the \"projected\" value\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    values = sigmoid(-(pos_distance - gt_distance_tiled))\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    pos_distance = torch.tensor(100)\n",
    "    gt_distance = torch.tensor(100) \n",
    "    value = sigmoid(-(pos_distance- gt_distance))\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data for testing\n",
    "points = loadData()\n",
    "dataset = prepareData(points)\n",
    "test_batch = dataset[128:256,:]\n",
    "ground_truth_distance = test_batch[:,0]\n",
    "angles = test_batch[:,1:3]\n",
    "origin = test_batch[:,3:]\n",
    "pos, ang = get_sample_positions(origin, angles,num_bins=100)\n",
    "\n",
    "model = LiDAR_NeRF(hidden_dim=256)\n",
    "rendered = model(pos, ang)\n",
    "sigmoid = nn.Sigmoid()\n",
    "rendered_sigmoid = sigmoid(rendered)\n",
    "temp = torch.zeros_like(pos)\n",
    "val = (get_actual_value(pos, ground_truth_distance)).to(dtype = torch.float32)\n",
    "\n",
    "\n",
    "# for x in val:\n",
    "#     print(x)\n",
    "# print(min(rendered))\n",
    "# loss_bce = nn.BCELoss()\n",
    "# loss = loss_bce(rendered_sigmoid, val)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, dataloader, device = 'cpu', epoch = int(1e5), num_bins = 100):\n",
    "    training_losses = []\n",
    "    for _ in tqdm(range(epoch)):\n",
    "        for batch in dataloader:\n",
    "            # parse the batch\n",
    "            ground_truth_distance = batch[:,0]\n",
    "            angles = batch[:,1:3]\n",
    "            origin = batch[:,:3:7]\n",
    "            \n",
    "            sample_positions, sample_angles = get_sample_positions(origin, angles, num_bins=num_bins)\n",
    "            \n",
    "            rendered_value = model(sample_positions.to(device), sample_angles.to(device))\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            rendered_value_sigmoid = sigmoid(rendered_value)\n",
    "            actual_value_sigmoided = (get_actual_value(sample_positions.to(device), ground_truth_distance.to(device))).to(dtype = torch.float32)\n",
    "            \n",
    "            # loss = lossBCE(rendered_value, actual_value_sigmoided)  # + lossEikonal(model)\n",
    "            # loss_bce = nn.CrossEntropyLoss()\n",
    "            loss_bce = nn.BCELoss()\n",
    "            loss = loss_bce(rendered_value_sigmoid, actual_value_sigmoided)\n",
    "            # BCEWithLogitsLoss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_losses.append(loss.item())\n",
    "            # print(loss.item())\n",
    "            print(loss.item())\n",
    "        scheduler.step()\n",
    "    return training_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")\n",
    "points = loadData()\n",
    "print(\"loaded data\")\n",
    "data_matrix = prepareData(points)\n",
    "print(\"prepared data\")\n",
    "training_dataset = torch.from_numpy(data_matrix)\n",
    "data_loader = DataLoader(training_dataset, batch_size=1024, shuffle = True)\n",
    "model = LiDAR_NeRF(hidden_dim=512, embedding_dim_dir=10, device = device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2, 4, 8, 16], gamma=0.5)\n",
    "losses = train(model, optimizer, scheduler, data_loader, epoch = 20, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model\n",
    "torch.save(model.state_dict(), 'version1_trial1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LiDAR_NeRF(\n",
       "  (block1): Sequential(\n",
       "    (0): Linear(in_features=105, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Linear(in_features=617, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Load the model and try to \"visualize\" the model's datapoints\n",
    "model2 = LiDAR_NeRF(hidden_dim=512, embedding_dim_dir=10, device = 'cpu')\n",
    "model2.load_state_dict(torch.load('/home/ansonhon/anson/thesis/LiDAR_NeRF/local/models/version1_trial1.pth'))\n",
    "model2.eval()  # Set the model to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/2962768184.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  temp = torch.tensor(sample_positions**2)\n",
      "/tmp/ipykernel_7409/3574676403.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_tensor = torch.tensor(pos)\n",
      "/tmp/ipykernel_7409/3574676403.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ang_tensor = torch.tensor(ang)\n"
     ]
    }
   ],
   "source": [
    "# sample data for testing\n",
    "points = loadData()\n",
    "dataset = prepareData(points)\n",
    "test_batch = dataset[0:128,:]\n",
    "ground_truth_distance = test_batch[:,0]\n",
    "angles = test_batch[:,1:3]\n",
    "origin = test_batch[:,3:]\n",
    "pos, ang = get_sample_positions(origin, angles,num_bins=100)\n",
    "# ang[:,1] += 0.01\n",
    "model = LiDAR_NeRF(hidden_dim=256)\n",
    "rendered = model(pos, ang)\n",
    "sigmoid = nn.Sigmoid()\n",
    "rendered_sigmoid = sigmoid(rendered)\n",
    "# pos = torch.zeros_like(pos)\n",
    "val = (get_actual_value(pos, ground_truth_distance)).to(dtype = torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pos_tensor = torch.tensor(pos)\n",
    "    ang_tensor = torch.tensor(ang)\n",
    "    output = model2(pos_tensor, ang_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.399214  , 10.91817829, 13.53060871,  6.07898947, 11.7334501 ,\n",
       "        9.37332625,  8.71858306,  8.13654893,  8.60786704, 11.26260301,\n",
       "        6.61249818, 24.06703704, 15.54482499,  9.59759753, 27.90561933,\n",
       "       38.18257097, 49.65299743, 27.85560469, 12.63629208,  9.20088626,\n",
       "       15.04839681,  8.09782063,  7.96666677,  5.78550798, 37.44105097,\n",
       "        4.78177721, 13.00769447, 13.22598086, 12.09721827, 10.75251612,\n",
       "        9.25793713, 24.2170193 , 13.9909485 ,  8.85495899,  9.80638234,\n",
       "       49.63953668, 21.07629429, 43.70530456,  6.25467492, 33.22301242,\n",
       "        9.33456179,  6.39301065, 48.02704691, 23.29812176,  4.17582805,\n",
       "       13.57226488, 18.06122046,  7.88255098, 10.0327647 , 18.31223302,\n",
       "        5.91224103, 12.04290148, 15.78143721, 12.58434868, 16.93181907,\n",
       "       10.78562153,  7.13742712, 19.47748405, 23.78903986,  6.67717171,\n",
       "       11.44818246, 11.44713756, 20.59084911,  4.50917099, 36.92286079,\n",
       "       15.31491297,  6.34567305, 15.13709847, 31.7577729 ,  6.55110849,\n",
       "       14.1766872 , 33.93032215, 40.30843064, 13.05559618, 16.34571656,\n",
       "       22.4093526 , 12.86785855,  8.84925565, 10.09933529, 18.57246115,\n",
       "       23.12021463, 14.67214825, 34.14015456, 31.14688106, 11.86048795,\n",
       "       11.58627925, 16.84166164, 11.11649224,  6.47469333, 11.37357401,\n",
       "       19.37642598, 12.00676906, 10.29031011, 21.23577674,  8.29601591,\n",
       "       14.60039267, 35.49199167, 28.96416878, 10.8833557 , 37.96336443,\n",
       "        7.97066663,  4.2185032 ,  7.11916782, 44.33565   , 12.78581019,\n",
       "        7.26831601, 14.49289394, 31.4721046 , 48.38402794, 15.51204036,\n",
       "        6.54165269,  7.69110906,  7.21911976,  7.79806193,  5.65628157,\n",
       "        7.53984124, 31.5214893 , 12.15965378,  6.99089242, 47.58827312,\n",
       "       24.43215807,  8.50587347,  8.4187877 , 33.06710569, 14.17791869,\n",
       "       15.95151072, 17.7087446 , 16.32354788])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  3.1185,  -0.5239,  -0.6558],\n",
      "        [  3.5806,  -0.6015,  -0.7530],\n",
      "        [  4.5801,  -0.7695,  -0.9632],\n",
      "        [  5.0458,  -0.8477,  -1.0611],\n",
      "        [  5.6898,  -0.9559,  -1.1966],\n",
      "        [  6.1265,  -1.0293,  -1.2884],\n",
      "        [  7.3240,  -1.2304,  -1.5403],\n",
      "        [  7.7609,  -1.3038,  -1.6322],\n",
      "        [  8.3259,  -1.3987,  -1.7510],\n",
      "        [  9.4974,  -1.5956,  -1.9973],\n",
      "        [ 10.1525,  -1.7056,  -2.1351],\n",
      "        [ 10.6502,  -1.7892,  -2.2398],\n",
      "        [ 11.1054,  -1.8657,  -2.3355],\n",
      "        [ 12.0142,  -2.0184,  -2.5266],\n",
      "        [ 12.3803,  -2.0799,  -2.6036],\n",
      "        [ 13.4859,  -2.2656,  -2.8361],\n",
      "        [ 14.3389,  -2.4089,  -3.0155],\n",
      "        [ 15.0206,  -2.5235,  -3.1589],\n",
      "        [ 15.5355,  -2.6100,  -3.2672],\n",
      "        [ 15.9780,  -2.6843,  -3.3602],\n",
      "        [ 16.9162,  -2.8419,  -3.5576],\n",
      "        [ 17.9054,  -3.0081,  -3.7656],\n",
      "        [ 18.4689,  -3.1028,  -3.8841],\n",
      "        [ 19.0608,  -3.2022,  -4.0086],\n",
      "        [ 19.8788,  -3.3396,  -4.1806],\n",
      "        [ 20.6663,  -3.4719,  -4.3462],\n",
      "        [ 20.9212,  -3.5148,  -4.3998],\n",
      "        [ 22.1533,  -3.7217,  -4.6589],\n",
      "        [ 22.5137,  -3.7823,  -4.7347],\n",
      "        [ 22.9465,  -3.8550,  -4.8258],\n",
      "        [ 24.2781,  -4.0787,  -5.1058],\n",
      "        [ 24.5152,  -4.1186,  -5.1557],\n",
      "        [ 25.6408,  -4.3076,  -5.3924],\n",
      "        [ 26.1917,  -4.4002,  -5.5082],\n",
      "        [ 26.4967,  -4.4514,  -5.5724],\n",
      "        [ 27.3276,  -4.5910,  -5.7471],\n",
      "        [ 28.1997,  -4.7376,  -5.9305],\n",
      "        [ 28.7918,  -4.8370,  -6.0551],\n",
      "        [ 29.3875,  -4.9371,  -6.1803],\n",
      "        [ 30.3057,  -5.0914,  -6.3734],\n",
      "        [ 31.0424,  -5.2151,  -6.5284],\n",
      "        [ 32.0360,  -5.3820,  -6.7373],\n",
      "        [ 32.1906,  -5.4080,  -6.7699],\n",
      "        [ 33.2505,  -5.5861,  -6.9927],\n",
      "        [ 33.8638,  -5.6891,  -7.1217],\n",
      "        [ 34.4351,  -5.7851,  -7.2419],\n",
      "        [ 35.3369,  -5.9366,  -7.4315],\n",
      "        [ 35.5651,  -5.9749,  -7.4795],\n",
      "        [ 36.3431,  -6.1056,  -7.6431],\n",
      "        [ 37.3752,  -6.2790,  -7.8602],\n",
      "        [ 38.0241,  -6.3880,  -7.9966],\n",
      "        [ 38.5047,  -6.4688,  -8.0977],\n",
      "        [ 39.2725,  -6.5978,  -8.2592],\n",
      "        [ 39.9526,  -6.7120,  -8.4022],\n",
      "        [ 40.5309,  -6.8092,  -8.5238],\n",
      "        [ 41.7367,  -7.0118,  -8.7774],\n",
      "        [ 42.0164,  -7.0587,  -8.8362],\n",
      "        [ 42.9382,  -7.2136,  -9.0301],\n",
      "        [ 43.5829,  -7.3219,  -9.1657],\n",
      "        [ 44.2095,  -7.4272,  -9.2975],\n",
      "        [ 44.9471,  -7.5511,  -9.4526],\n",
      "        [ 45.4499,  -7.6356,  -9.5583],\n",
      "        [ 46.4144,  -7.7976,  -9.7612],\n",
      "        [ 47.1006,  -7.9129,  -9.9055],\n",
      "        [ 47.8911,  -8.0457, -10.0717],\n",
      "        [ 48.2699,  -8.1093, -10.1514],\n",
      "        [ 48.9044,  -8.2159, -10.2848],\n",
      "        [ 49.6224,  -8.3365, -10.4358],\n",
      "        [ 50.5400,  -8.4907, -10.6288],\n",
      "        [ 51.6042,  -8.6695, -10.8526],\n",
      "        [ 52.2783,  -8.7827, -10.9944],\n",
      "        [ 52.7991,  -8.8702, -11.1039],\n",
      "        [ 53.1586,  -8.9306, -11.1795],\n",
      "        [ 53.9608,  -9.0654, -11.3482],\n",
      "        [ 54.7747,  -9.2021, -11.5194],\n",
      "        [ 55.7601,  -9.3677, -11.7266],\n",
      "        [ 56.2252,  -9.4458, -11.8244],\n",
      "        [ 56.6756,  -9.5215, -11.9192],\n",
      "        [ 57.4663,  -9.6543, -12.0854],\n",
      "        [ 58.7054,  -9.8625, -12.3460],\n",
      "        [ 59.1582,  -9.9386, -12.4413],\n",
      "        [ 59.5343, -10.0017, -12.5204],\n",
      "        [ 60.5305, -10.1691, -12.7299],\n",
      "        [ 61.4337, -10.3209, -12.9198],\n",
      "        [ 61.6354, -10.3547, -12.9622],\n",
      "        [ 62.7679, -10.5450, -13.2004],\n",
      "        [ 63.6258, -10.6891, -13.3808],\n",
      "        [ 64.2469, -10.7935, -13.5114],\n",
      "        [ 64.6847, -10.8670, -13.6035],\n",
      "        [ 65.6011, -11.0210, -13.7962],\n",
      "        [ 65.9160, -11.0739, -13.8625],\n",
      "        [ 66.4629, -11.1658, -13.9775],\n",
      "        [ 67.5755, -11.3527, -14.2115],\n",
      "        [ 68.2614, -11.4679, -14.3557],\n",
      "        [ 68.8715, -11.5704, -14.4840],\n",
      "        [ 69.4225, -11.6630, -14.5999],\n",
      "        [ 70.2663, -11.8047, -14.7774],\n",
      "        [ 70.8964, -11.9106, -14.9099],\n",
      "        [ 71.4374, -12.0015, -15.0236],\n",
      "        [ 72.2279, -12.1343, -15.1899]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(pos[0:100,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.2275e-01],\n",
      "        [-1.0635e+00],\n",
      "        [-1.3142e+00],\n",
      "        [-1.7705e+00],\n",
      "        [-2.5884e+00],\n",
      "        [-3.0063e+00],\n",
      "        [-3.6417e+00],\n",
      "        [-4.4365e+00],\n",
      "        [-4.8669e+00],\n",
      "        [-5.4820e+00],\n",
      "        [-6.4632e+00],\n",
      "        [-6.6659e+00],\n",
      "        [-7.2081e+00],\n",
      "        [-8.4017e+00],\n",
      "        [-8.9906e+00],\n",
      "        [-9.6592e+00],\n",
      "        [-1.0285e+01],\n",
      "        [-1.0843e+01],\n",
      "        [-1.1799e+01],\n",
      "        [-1.3038e+01],\n",
      "        [-1.3060e+01],\n",
      "        [-1.3554e+01],\n",
      "        [-1.4500e+01],\n",
      "        [-1.5559e+01],\n",
      "        [-1.6112e+01],\n",
      "        [-1.6187e+01],\n",
      "        [-1.6843e+01],\n",
      "        [-1.7287e+01],\n",
      "        [-1.7841e+01],\n",
      "        [-1.8358e+01],\n",
      "        [-1.9024e+01],\n",
      "        [-1.9419e+01],\n",
      "        [-2.0323e+01],\n",
      "        [-2.1107e+01],\n",
      "        [-2.1104e+01],\n",
      "        [-2.1432e+01],\n",
      "        [-2.1914e+01],\n",
      "        [-2.2501e+01],\n",
      "        [-2.2534e+01],\n",
      "        [-2.3445e+01],\n",
      "        [-2.3725e+01],\n",
      "        [-2.4523e+01],\n",
      "        [-2.4248e+01],\n",
      "        [-2.4803e+01],\n",
      "        [-2.4973e+01],\n",
      "        [-2.5085e+01],\n",
      "        [-2.5937e+01],\n",
      "        [-2.6064e+01],\n",
      "        [-2.6358e+01],\n",
      "        [-2.7235e+01],\n",
      "        [-2.7222e+01],\n",
      "        [-2.7837e+01],\n",
      "        [-2.7893e+01],\n",
      "        [-2.8010e+01],\n",
      "        [-2.8387e+01],\n",
      "        [-2.9011e+01],\n",
      "        [-2.9526e+01],\n",
      "        [-2.9669e+01],\n",
      "        [-2.9999e+01],\n",
      "        [-3.0660e+01],\n",
      "        [-3.1099e+01],\n",
      "        [-3.1626e+01],\n",
      "        [-3.1983e+01],\n",
      "        [-3.2298e+01],\n",
      "        [-3.2714e+01],\n",
      "        [-3.3777e+01],\n",
      "        [-3.4219e+01],\n",
      "        [-3.4872e+01],\n",
      "        [-3.5240e+01],\n",
      "        [-3.5424e+01],\n",
      "        [-3.6119e+01],\n",
      "        [-3.6237e+01],\n",
      "        [-3.6689e+01],\n",
      "        [-3.6788e+01],\n",
      "        [-3.7754e+01],\n",
      "        [-3.8126e+01],\n",
      "        [-3.8609e+01],\n",
      "        [-3.9685e+01],\n",
      "        [-4.0291e+01],\n",
      "        [-4.1006e+01],\n",
      "        [-4.1199e+01],\n",
      "        [-4.2467e+01],\n",
      "        [-4.2468e+01],\n",
      "        [-4.2422e+01],\n",
      "        [-4.3509e+01],\n",
      "        [-4.4146e+01],\n",
      "        [-4.4537e+01],\n",
      "        [-4.4797e+01],\n",
      "        [-4.5180e+01],\n",
      "        [-4.5993e+01],\n",
      "        [-4.6473e+01],\n",
      "        [-4.7312e+01],\n",
      "        [-4.8182e+01],\n",
      "        [-4.7967e+01],\n",
      "        [-4.8655e+01],\n",
      "        [-4.9427e+01],\n",
      "        [-4.9855e+01],\n",
      "        [-5.0886e+01],\n",
      "        [-5.0772e+01],\n",
      "        [-5.0109e+01],\n",
      "        [ 4.7876e-01],\n",
      "        [ 2.9359e-01],\n",
      "        [ 2.9420e-02],\n",
      "        [-1.4574e-01],\n",
      "        [-3.7174e-01],\n",
      "        [-8.8851e-01],\n",
      "        [-1.2901e+00],\n",
      "        [-1.9961e+00],\n",
      "        [-2.5571e+00],\n",
      "        [-2.4716e+00],\n",
      "        [-2.6389e+00],\n",
      "        [-3.6088e+00],\n",
      "        [-4.6853e+00],\n",
      "        [-4.9997e+00],\n",
      "        [-6.0789e+00],\n",
      "        [-6.0310e+00],\n",
      "        [-6.2972e+00],\n",
      "        [-6.9677e+00],\n",
      "        [-7.9867e+00],\n",
      "        [-8.2893e+00],\n",
      "        [-8.8150e+00],\n",
      "        [-8.9374e+00],\n",
      "        [-9.4911e+00],\n",
      "        [-9.8220e+00],\n",
      "        [-1.0573e+01],\n",
      "        [-1.0731e+01],\n",
      "        [-1.1559e+01],\n",
      "        [-1.1615e+01],\n",
      "        [-1.1410e+01],\n",
      "        [-1.2225e+01],\n",
      "        [-1.2814e+01],\n",
      "        [-1.3485e+01],\n",
      "        [-1.3690e+01],\n",
      "        [-1.4157e+01],\n",
      "        [-1.4252e+01],\n",
      "        [-1.4377e+01],\n",
      "        [-1.4514e+01],\n",
      "        [-1.4987e+01],\n",
      "        [-1.5567e+01],\n",
      "        [-1.5549e+01],\n",
      "        [-1.6462e+01],\n",
      "        [-1.6548e+01],\n",
      "        [-1.7451e+01],\n",
      "        [-1.7477e+01],\n",
      "        [-1.7586e+01],\n",
      "        [-1.7983e+01],\n",
      "        [-1.7942e+01],\n",
      "        [-1.7853e+01],\n",
      "        [-1.8689e+01],\n",
      "        [-1.8539e+01],\n",
      "        [-1.8960e+01],\n",
      "        [-1.9047e+01],\n",
      "        [-1.9696e+01],\n",
      "        [-2.0748e+01],\n",
      "        [-2.0638e+01],\n",
      "        [-2.0891e+01],\n",
      "        [-2.1364e+01],\n",
      "        [-2.1561e+01],\n",
      "        [-2.1887e+01],\n",
      "        [-2.2131e+01],\n",
      "        [-2.2338e+01],\n",
      "        [-2.2972e+01],\n",
      "        [-2.2986e+01],\n",
      "        [-2.3357e+01],\n",
      "        [-2.3314e+01],\n",
      "        [-2.4421e+01],\n",
      "        [-2.4824e+01],\n",
      "        [-2.4787e+01],\n",
      "        [-2.5247e+01],\n",
      "        [-2.5508e+01],\n",
      "        [-2.6167e+01],\n",
      "        [-2.6191e+01],\n",
      "        [-2.6001e+01],\n",
      "        [-2.6249e+01],\n",
      "        [-2.7028e+01],\n",
      "        [-2.6876e+01],\n",
      "        [-2.7815e+01],\n",
      "        [-2.8296e+01],\n",
      "        [-2.8555e+01],\n",
      "        [-2.8605e+01],\n",
      "        [-2.9135e+01],\n",
      "        [-2.9509e+01],\n",
      "        [-2.9748e+01],\n",
      "        [-3.0125e+01],\n",
      "        [-3.0516e+01],\n",
      "        [-3.0687e+01],\n",
      "        [-3.0981e+01],\n",
      "        [-3.1164e+01],\n",
      "        [-3.1492e+01],\n",
      "        [-3.2529e+01],\n",
      "        [-3.2552e+01],\n",
      "        [-3.2759e+01],\n",
      "        [-3.3262e+01],\n",
      "        [-3.3211e+01],\n",
      "        [-3.4037e+01],\n",
      "        [-3.4220e+01],\n",
      "        [-3.4479e+01],\n",
      "        [-3.4267e+01],\n",
      "        [-3.4997e+01],\n",
      "        [-3.4606e+01]])\n"
     ]
    }
   ],
   "source": [
    "print(output[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dist = 1 # initial distanc forvisualization\n",
    "    pos = torch.zeros((100000,3))\n",
    "    ele = torch.linspace(-0.34, 0.3, 100)\n",
    "    pan = torch.linspace(-3.14, 3.14, 1000)\n",
    "    ele_tiled = repeat(ele, 'n -> (r n) 1', r = 1000)\n",
    "    pan_tiled = repeat(pan, 'n -> (n r) 1', r = 100)\n",
    "    ang = torch.cat((ele_tiled, pan_tiled), dim=1)\n",
    "\n",
    "    # direction for each \"point\" from camera centre\n",
    "    directions = torch.tensor(sph2cart(np.array(ang)))\n",
    "\n",
    "    \n",
    "\n",
    "    # for i in range(50):\n",
    "    #     output2 = model2(pos, ang)\n",
    "    #     temp = torch.sign(output2)\n",
    "    #     pos += directions * dist * temp\n",
    "    #     # dist /= 2\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Version  Slot ID  LiDAR Index  Rsvd  Error Code  Timestamp Type  Data Type  \\\n",
      "0        5        7            1     0  0x00000200               0          0   \n",
      "1        5        7            1     0  0x00000200               0          0   \n",
      "2        5        7            1     0  0x00000200               0          0   \n",
      "3        5        7            1     0  0x00000200               0          0   \n",
      "4        5        7            1     0  0x00000200               0          0   \n",
      "\n",
      "      Timestamp         X         Y         Z  Reflectivity  Tag  Ori_x  \\\n",
      "0  339330000000 -7.542027 -0.012011 -2.667897            24    0   8128   \n",
      "1  339330000000 -7.559117 -0.012038 -2.619085            24    0   8132   \n",
      "2  339330000000 -5.681918 -0.009049 -1.927623            24    0   8137   \n",
      "3  339330000000 -5.694260 -0.009068 -1.890851            28    0   8132   \n",
      "4  339330000000 -5.706365 -0.009088 -1.854000            28    0   8132   \n",
      "\n",
      "   Ori_y  Ori_z  Ori_radius  Ori_theta  Ori_phi  \n",
      "0  23651   6826           0          0        0  \n",
      "1  23653   6788           0          0        0  \n",
      "2  23661   6751           0          0        0  \n",
      "3  23646   6707           0          0        0  \n",
      "4  23642   6665           0          0        0  \n"
     ]
    }
   ],
   "source": [
    "### Save to csv for visualization\n",
    "df_temp = pd.read_csv('datasets/testing1/testing.csv')\n",
    "df_temp = df_temp.head(100000)\n",
    "pos_np = pos.numpy()\n",
    "df_temp['X'] = pos_np[:,0]\n",
    "df_temp['Y'] = pos_np[:,1]\n",
    "df_temp['Z'] = pos_np[:,2]\n",
    "print(df_temp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.to_csv('testing.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidar_nerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
