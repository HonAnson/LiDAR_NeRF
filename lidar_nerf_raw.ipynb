{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "from einops import rearrange\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from scipy.special import sph_harm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperation (NOTE: Using meter as unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pointcloud from cartisean coordinate to spherical coordinate\n",
    "def cart2sph(xyz):\n",
    "    x = xyz[:,0]\n",
    "    y = xyz[:,1]\n",
    "    z = xyz[:,2]\n",
    "    XsqPlusYsq = x**2 + y**2\n",
    "    r = np.sqrt(list(XsqPlusYsq + z**2))\n",
    "    elev = np.arctan2(list(z), np.sqrt(list(XsqPlusYsq)))\n",
    "    pan = np.arctan2(list(x), list(y))\n",
    "    output = np.array([r, elev, pan])\n",
    "    return rearrange(output, 'a b -> b a') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path\n",
    "dataset_path = 'datasets/testing1'\n",
    "\n",
    "# List all files in the specified path, ignoring directories\n",
    "files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "files.sort()\n",
    "\n",
    "# read the files\n",
    "points_xyz = []\n",
    "for s in files:\n",
    "    path = 'datasets/testing1/' + s\n",
    "    df = pd.read_csv(path)\n",
    "    a = df.to_numpy()\n",
    "    b = a[:,8:11]\n",
    "    points_xyz.append(b)\n",
    "\n",
    "# Now we can find the fiew direction of each points:\n",
    "# NOTE: points in spherical coordinate are arranged: [r, elev, pan]\n",
    "points_sphere = []\n",
    "for points in points_xyz:\n",
    "    points_sphere.append(cart2sph(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we now process the data\n",
    "# Translation vectors for points in each view, we are using camera centre at first frame as origin of world coordinate\n",
    "# NOTE: translation vectors below are found by assuming transformation between frames are translations, and obatined by manually finding corrspondance\n",
    "# They are translation of the same corrspondance across different frames\n",
    "# HARD CODED HERE\n",
    "t0 = np.array([0,0,0])\n",
    "t1 = np.array([-0.671,-0.016,0.215])\n",
    "t2 = np.array([-1.825,-0.091,0.147])\n",
    "t3 = np.array([-2.661,-0.263,0.166])\n",
    "t4 = np.array([-3.607,-0.156,0.039])\n",
    "translations = [t0, t1, t2, t3, t4]\n",
    "\n",
    "# camera centre locations\n",
    "centres = [-t for t in translations]\n",
    "centres_data = []\n",
    "for i,c in enumerate(centres):\n",
    "    l = len(points_sphere[i])\n",
    "    temp = np.tile(c, (l, 1))\n",
    "    centres_data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the points into one big matrix\n",
    "stacked = []\n",
    "for i in range(len(points_sphere)):\n",
    "    temp = np.hstack((points_sphere[i], centres_data[i]))\n",
    "    stacked.append(temp)\n",
    "\n",
    "dataset = np.array([])\n",
    "for i in range(len(stacked)):\n",
    "    if i == 0:\n",
    "        dataset = stacked[i]\n",
    "    else:\n",
    "        dataset = np.vstack((dataset, stacked[i]))\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Filter out points where the distance value is = 0\n",
    "mask1 = dataset[:, 0] != 0\n",
    "mask2 = dataset[:,0] < 50\n",
    "mask = mask1 + mask2\n",
    "dataset = dataset[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ele = dataset[:,1]\n",
    "pan = dataset[:,2]\n",
    "ele = ele + np.pi / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum degree of harmonics\n",
    "l_max = 8\n",
    "num_features = sum(2 * l + 1 for l in range(l_max + 1))\n",
    "features = np.zeros((dataset.shape[0], num_features))\n",
    "feature_idx = 0\n",
    "for l in range(l_max + 1):\n",
    "    for m in range(-l, l + 1):\n",
    "        Y_lm = sph_harm(m, l, pan, ele)\n",
    "        features[:, feature_idx] = Y_lm.real  # Storing real part, or use absolute values, etc.\n",
    "        feature_idx += 1\n",
    "\n",
    "a = rearrange(dataset[:,0], 'a -> a 1')\n",
    "encoded_data = np.hstack((a,features, dataset[:,3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now prepare to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to pytorch tensor\n",
    "X = np.array(encoded_data[:,1:])\n",
    "y = np.array(encoded_data[:,0])\n",
    "\n",
    "# Convert to tensor:\n",
    "X_tensor = torch.from_numpy(X).double()\n",
    "y_tensor = torch.from_numpy(y).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we prepare to train the model\n",
    "features = 5\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(features, 512),  # Input layer with 5 inputs and 10 outputs\n",
    "            nn.ReLU(),                # Activation function\n",
    "            nn.Linear(512, 512),        # Hidden layer with 512 neurons\n",
    "            nn.ReLU(),                # Activation function\n",
    "            nn.Linear(512, 512),        # Hidden layer with 512 neurons\n",
    "            nn.ReLU(),                # Activation function\n",
    "            nn.Linear(512, 512),        # Hidden layer with 512 neurons\n",
    "            nn.ReLU(),                # Activation function\n",
    "            nn.Linear(512, 512),        # Hidden layer with 512 neurons\n",
    "            nn.ReLU(),                # Activation function\n",
    "            nn.Linear(512, 1)          # Output layer with 1 output\n",
    "        )\n",
    "    \n",
    "    @staticmethod                               \n",
    "    def sph_harm_emb(x, L):\n",
    "        pan = x[0]\n",
    "        ele = x[1]\n",
    "        out = []\n",
    "        out.append(pan)\n",
    "        out.append(ele)\n",
    "        num_features = sum(2 * l + 1 for l in range(L + 1))\n",
    "        for l in range(L + 1):\n",
    "            for m in range(-l, l + 1):\n",
    "                Y_lm = sph_harm(m, l, pan, ele)\n",
    "                out.append(Y_lm.real)  \n",
    "        return torch.tensor(out)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        angles = x[0:2]\n",
    "        origin = x[2:]\n",
    "        emb_angle = self.sph_harm_emb(angles, 8)\n",
    "        temp = torch.cat(emb_angle, origin, axis=0)\n",
    "        return self.layers(temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "batch_size = 256\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "# Initialize the model\n",
    "model = MLP().to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device).float(), targets.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidar_nerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
