{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "from einops import rearrange, repeat\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "#test update for branch wild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperation (NOTE: Using meter as unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "# convert pointcloud from cartisean coordinate to spherical coordinate\n",
    "def cart2sph(xyz):\n",
    "    x = xyz[:,0]\n",
    "    y = xyz[:,1]\n",
    "    z = xyz[:,2]\n",
    "    XsqPlusYsq = x**2 + y**2\n",
    "    r = np.sqrt(list(XsqPlusYsq + z**2))\n",
    "    elev = np.arctan2(list(z), np.sqrt(list(XsqPlusYsq)))\n",
    "    pan = np.arctan2(list(x), list(y))\n",
    "\n",
    "    output = np.array([r, elev, pan])\n",
    "    return rearrange(output, 'a b -> b a') #take transpose\n",
    "\n",
    "\n",
    "def sph2cart(ang):\n",
    "    ele = ang[:,0]\n",
    "    pan = ang[:,1]\n",
    "    x = np.cos(ele)*np.cos(pan)\n",
    "    y = np.cos(ele)*np.sin(pan)\n",
    "    z = np.sin(ele)\n",
    "    output = np.array([x,y,z])\n",
    "    return rearrange(output, 'a b -> b a') #take transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    # Specify the directory path\n",
    "    dataset_path = 'datasets/testing1'\n",
    "\n",
    "    # List all files in the specified path, ignoring directories\n",
    "    files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "    files.sort()\n",
    "\n",
    "    # read the files\n",
    "    points_xyz = []\n",
    "    for s in files:\n",
    "        path = 'datasets/testing1/' + s\n",
    "        df = pd.read_csv(path)\n",
    "        a = df.to_numpy()\n",
    "        points_xyz.append(a[:,8:11])\n",
    "    return points_xyz\n",
    "\n",
    "def prepareData(points_xyz):\n",
    "    # Find the fiew direction of each points:\n",
    "    # NOTE: points in spherical coordinate are arranged: [r, elev, pan]\n",
    "    points_sphere = []\n",
    "    for points in points_xyz:\n",
    "        points_sphere.append(cart2sph(points))\n",
    "\n",
    "    ### Process the data\n",
    "    # Translation vectors for points in each view, we are using camera centre at first frame as origin of world coordinate\n",
    "    # NOTE: translation vectors below are found by assuming transformation between frames are translations, and obatined by manually finding corrspondance between frames\n",
    "    # They are translation of the same corrspondance across different frames\n",
    "    # HARD CODED HERE...\n",
    "    t0 = np.array([0,0,0])\n",
    "    t1 = np.array([-0.671,-0.016,0.215])\n",
    "    t2 = np.array([-1.825,-0.091,0.147])\n",
    "    t3 = np.array([-2.661,-0.263,0.166])\n",
    "    t4 = np.array([-3.607,-0.156,0.039])\n",
    "    # ...TO HERE\n",
    "    translations = [t0, t1, t2, t3, t4]\n",
    "\n",
    "    # camera centre locations\n",
    "    centres = [-t for t in translations]\n",
    "    centres_data = []\n",
    "    for i,c in enumerate(centres):\n",
    "        l = len(points_sphere[i])\n",
    "        temp = np.tile(c, (l, 1))\n",
    "        centres_data.append(temp)\n",
    "\n",
    "    # stack the points into one big matrix\n",
    "    stacked = []\n",
    "    for i in range(len(points_sphere)):\n",
    "        temp = np.hstack((points_sphere[i], centres_data[i]))\n",
    "        stacked.append(temp)\n",
    "\n",
    "    dataset = np.array([])\n",
    "    for i in range(len(stacked)):\n",
    "        if i == 0:\n",
    "            dataset = stacked[i]\n",
    "        else:\n",
    "            dataset = np.vstack((dataset, stacked[i]))\n",
    "\n",
    "\n",
    "    np.random.shuffle(dataset)\n",
    "\n",
    "    # Mid pass filter, for distance value between 2 and 50 meter\n",
    "    mask1 = dataset[:,0] > 2\n",
    "    dataset = dataset[mask1]\n",
    "    mask2 = dataset[:,0] < 50\n",
    "    dataset = dataset[mask2]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiDAR_NeRF(nn.Module):\n",
    "    def __init__(self, embedding_dim_pos = 10, embedding_dim_dir = 4, hidden_dim = 256, device = 'cuda'):\n",
    "        super(LiDAR_NeRF, self).__init__()\n",
    "        self.device = device\n",
    "        self.embedding_dim_dir = embedding_dim_dir\n",
    "        self.embedding_dim_pos = embedding_dim_pos\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim_pos * 6 + 3 + embedding_dim_dir * 4 + 2, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim_pos * 6 + 3 + embedding_dim_dir * 4 + 2 + hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def positional_encoding(x, L):\n",
    "        out = [x]\n",
    "        for j in range(L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    def forward(self, o, d):\n",
    "        emb_x = self.positional_encoding(o, self.embedding_dim_pos)\n",
    "        emb_d = self.positional_encoding(d, self.embedding_dim_dir)\n",
    "        input = torch.hstack((emb_x,emb_d)).to(dtype=torch.float32)\n",
    "        temp = self.block1(input)\n",
    "        input2 = torch.hstack((temp, input)).to(dtype=torch.float32) # add skip input\n",
    "        output = self.block2(input2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSamples(origins, angles, ground_truth_distance, num_bins = 100):\n",
    "    elev = angles[:,0]\n",
    "    pan = angles[:,1]\n",
    "    dir_x = torch.tensor(np.cos(elev)*np.cos(pan))      # [batch_size]\n",
    "    dir_y = torch.tensor(np.cos(elev)*np.sin(pan))      # [batch_size]\n",
    "    dir_z = torch.tensor(np.sin(elev))\n",
    "    gt_tensor = torch.tensor(ground_truth_distance)\n",
    "    # create a list of magnitudes with even spacing from 0 to 1\n",
    "    t = torch.linspace(0,1, num_bins).expand(dir_x.shape[0], num_bins)  # [batch_size, num_bins]\n",
    "    \n",
    "    # preterb the spacing\n",
    "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "    lower = torch.cat((t[:, :1], mid), -1)\n",
    "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape)\n",
    "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "    t = rearrange(t, 'a b -> b a')  # [num_bins, batch_size]  take transpose so that multiplication can broadcast\n",
    "    \n",
    "    # multiply the magnitude to ground truth distance and add 3 meter\n",
    "    t = torch.sqrt(t)\n",
    "    t = gt_tensor*t\n",
    "    t += 5\n",
    "\n",
    "    # convert magnitudes into positions by multiplying it to the unit vector\n",
    "    pos_x = dir_x*t     # [num_bins, batch_size]\n",
    "    pos_y = dir_y*t\n",
    "    pos_z = dir_z*t\n",
    "    # concat them for output\n",
    "    multiplied = rearrange([pos_x,pos_y,pos_z], 'c b n  -> (n b) c')   # [num_bin*batchsize, 3]\n",
    "    # tile the origin values\n",
    "    origins_tiled = repeat(origins, 'n c -> (n b) c', b = num_bins) # [num_bin*batch_size, 3]\n",
    "    pos = torch.tensor(origins_tiled) + multiplied\n",
    "    # tile the angle too\n",
    "    angles_tiled = torch.tensor(repeat(angles, 'n c -> (n b) c', b = num_bins))\n",
    "    return pos, angles_tiled, origins_tiled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    if i == 2:\n",
    "        continue\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUpSamples(origins, angles, gt_distance, num_rolls = 1):\n",
    "    upsample_pos = torch.empty(0,3)\n",
    "    upsample_ang = torch.empty(0,2)\n",
    "    upsample_gt_dist = torch.empty(0,1)\n",
    "\n",
    "    for num_roll in range(1, num_rolls+1):\n",
    "        # first, we prepare pairs of data, where one of them has a shorter ray, and another has a longer ray\n",
    "        gt_distance_rolled = torch.roll(gt_distance, num_roll, 0)\n",
    "        condition =  gt_distance < gt_distance_rolled\n",
    "        condition = rearrange(condition, 'a -> a 1')\n",
    "\n",
    "        dir = torch.tensor(sph2cart(angles))\n",
    "        gt_dist = rearrange(gt_distance, 'a -> a 1')\n",
    "        gt_distance_rolled = rearrange(gt_distance_rolled, 'a -> a 1')\n",
    "        pos = gt_dist * dir\n",
    "\n",
    "        pos_shorter = torch.where(condition, pos, torch.roll(pos, num_roll, 0))\n",
    "        origins_shorter = torch.where(condition, origins, torch.roll(origins, num_roll, 0))\n",
    "        angles_shorter = torch.where(condition, angles, torch.roll(angles, num_roll, 0))\n",
    "        gt_dist_shorter = torch.where(condition, gt_dist, gt_distance_rolled)\n",
    "        pos_longer = torch.where(condition, torch.roll(pos, num_roll, 0), pos)\n",
    "        origins_longer = torch.where(condition, torch.roll(origins, num_roll, 0), origins)\n",
    "        angles_longer = torch.where(condition, torch.roll(angles, num_roll, 0), angles)\n",
    "        gt_dist_longer = torch.where(condition, gt_distance_rolled, gt_dist)\n",
    "        \n",
    "        # check if angle between pairs are small\n",
    "        diff = torch.abs(angles_shorter - angles_longer)\n",
    "        mask_small_ang = (diff < 0.09).all(dim=1)   ### NOTE: hardcoded 0.09 radient difference max\n",
    "\n",
    "        # check if origin between pairs are small\n",
    "        diff2 = torch.abs(origins_shorter - origins_longer)\n",
    "        mask_small_org = (diff2 < 0.2).all(dim=1)   ### pass if coordinate in origin are less than 20cm in all dimensions\n",
    "        \n",
    "        # # check if the target location is close\n",
    "        # diff3 = torch.abs(pos_shorter - pos_longer)\n",
    "        # mask_small_dis = (diff3 < 0.2).all(dim=1)\n",
    "\n",
    "        # get masks for both cases\n",
    "        mask_same_org = mask_small_ang & mask_small_org\n",
    "        # mask_same_tag = mask_small_ang & mask_small_dis\n",
    "\n",
    "        ### Handling case of same origin\n",
    "        # prepare set where rays are to be upsampled \n",
    "        # ensuring samples are at rays that are longer\n",
    "        angles_from_same_org = angles_longer[mask_same_org]\n",
    "        origins_from_same_org = origins_longer[mask_same_org]\n",
    "        gt_dist_to_same_org = gt_dist_shorter[mask_same_org] \n",
    "        pos_to_same_org = pos_shorter[mask_same_org]\n",
    "\n",
    "        if angles_from_same_org.shape[0] == 0:\n",
    "            continue   # skip if there are no points available for upsampling\n",
    "\n",
    "        # calculate upsampling position\n",
    "        num_bins = 20 \n",
    "        t = torch.linspace(0,1, num_bins).expand(angles_from_same_org.shape[0], num_bins)  # [batch_size, num_bins]\n",
    "        \n",
    "        # preterb the spacing\n",
    "        mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "        lower = torch.cat((t[:, :1], mid), -1)\n",
    "        upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "        u = torch.rand(t.shape)\n",
    "        t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "        t = rearrange(t, 'a b -> b a')  # [num_bins, batch_size]  take transpose so that multiplication can broadcast\n",
    "        t = rearrange(t, 'a b -> (b a) 1')\n",
    "        t = torch.sqrt(t)\n",
    "        \n",
    "        # get the sampling positions\n",
    "        origins_from_tiled = repeat(origins_from_same_org, 'n c -> (n b) c', b = num_bins)\n",
    "        dir_from = torch.tensor(sph2cart(angles_from_same_org))\n",
    "        dir_from_tiled = repeat(dir_from, 'n c -> (n b) c', b = num_bins)\n",
    "        gt_dist_to_tiled = repeat(gt_dist_to_same_org, 'n c -> (n b) c', b = num_bins)      ### problem here\n",
    "\n",
    "        pos_from = origins_from_tiled + dir_from_tiled * t * gt_dist_to_tiled\n",
    "        pos_to_tiled = repeat(pos_to_same_org, 'n c -> (n b) c', b = num_bins)\n",
    "\n",
    "        # also calculte the ground truth distance of our up sampled location\n",
    "        sample_sph = cart2sph(pos_from - pos_to_tiled)\n",
    "        sample_direction = torch.tensor(sample_sph[:,1:])\n",
    "        sample_gt_distance = torch.tensor(sample_sph[:,0])\n",
    "\n",
    "        # add one more dimension to sample_gt_distance\n",
    "        sample_gt_distance = rearrange(sample_gt_distance, 'a -> a 1')\n",
    "\n",
    "        upsample_pos = torch.vstack((upsample_pos, pos_from))\n",
    "        upsample_ang = torch.vstack((upsample_ang, sample_direction))\n",
    "        upsample_gt_dist = torch.vstack((upsample_gt_dist, sample_gt_distance))\n",
    "\n",
    "    # return pos_from , torch.tensor(sample_direction), torch.tensor(sample_gt_distance)\n",
    "    return upsample_pos, upsample_ang, upsample_gt_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns pytorch tensor of sigmoid of projected SDF\n",
    "def getTargetValues(sample_positions, gt_distance, origins, num_bins=100):\n",
    "    # calculate distance from sample_position\n",
    "    temp = torch.tensor((sample_positions - origins)**2)\n",
    "    pos_distance = torch.sqrt(torch.sum(temp, dim=1, keepdim=True))\n",
    "    \n",
    "    # find the \"projected\" value\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    values = sigmoid(-(pos_distance - gt_distance))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "num_bins = 100\n",
    "device = 'cuda'\n",
    "\n",
    "# sample data for testing\n",
    "points = loadData()\n",
    "dataset = prepareData(points)\n",
    "test_batch = torch.tensor(dataset[0:256,:])\n",
    "gt_distance = test_batch[:,0]\n",
    "angles = test_batch[:,1:3]\n",
    "origins = test_batch[:,3:6]\n",
    "upsample_pos, upsample_ang, upsample_gt_distance = getUpSamples(origins, angles,  gt_distance, num_rolls=5)\n",
    "# sample_pos, sample_ang, sample_org = getSamples(origins, angles, gt_distance, num_bins = num_bins)\n",
    "# gt_distance_tiled = repeat(gt_distance, 'b -> (b n) 1', n = num_bins)\n",
    "\n",
    "# # stack the upsampled position to sampled positions\n",
    "# pos = (torch.vstack((sample_pos, upsample_pos))).to(device)\n",
    "# ang = (torch.vstack((sample_ang, upsample_ang))).to(device)\n",
    "# gt_dis = (torch.vstack((gt_distance_tiled,upsample_gt_distance) )).to(device)\n",
    "# org = (torch.vstack((sample_org, upsample_pos))).to(device)\n",
    "\n",
    "# # inference and training\n",
    "# model = LiDAR_NeRF(hidden_dim=512)\n",
    "# rendered_value = model(pos, ang)\n",
    "# sigmoid = nn.Sigmoid()\n",
    "# rendered_value_sigmoid = sigmoid(rendered_value)\n",
    "# actual_value_sigmoided = (getTargetValues(pos, gt_dis, org)).to(dtype = torch.float32)\n",
    "# # loss = lossBCE(rendered_value, actual_value_sigmoided)  # + lossEikonal(model)\n",
    "# loss_bce = nn.BCELoss()\n",
    "# loss = loss_bce(rendered_value_sigmoid, actual_value_sigmoided)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1019, -2.4075],\n",
       "        [ 0.1042, -2.4146],\n",
       "        [ 0.1090, -2.4295],\n",
       "        [ 0.1142, -2.4461],\n",
       "        [ 0.1229, -2.4736],\n",
       "        [ 0.1274, -2.4882],\n",
       "        [ 0.1358, -2.5155],\n",
       "        [ 0.1371, -2.5198],\n",
       "        [ 0.1452, -2.5464],\n",
       "        [ 0.1541, -2.5764],\n",
       "        [ 0.1701, -2.6314],\n",
       "        [ 0.1836, -2.6798],\n",
       "        [ 0.1937, -2.7168],\n",
       "        [ 0.2046, -2.7584],\n",
       "        [ 0.2283, -2.8545],\n",
       "        [ 0.2584, -2.9925],\n",
       "        [ 0.2951,  3.0750],\n",
       "        [ 0.3141,  2.9096],\n",
       "        [ 0.3263,  2.5144],\n",
       "        [ 0.3194,  2.3770],\n",
       "        [ 0.0896,  2.8278],\n",
       "        [ 0.0896,  2.8183],\n",
       "        [ 0.0895,  2.8122],\n",
       "        [ 0.0894,  2.7957],\n",
       "        [ 0.0893,  2.7866],\n",
       "        [ 0.0891,  2.7738],\n",
       "        [ 0.0891,  2.7718],\n",
       "        [ 0.0889,  2.7569],\n",
       "        [ 0.0887,  2.7434],\n",
       "        [ 0.0882,  2.7131],\n",
       "        [ 0.0877,  2.6875],\n",
       "        [ 0.0874,  2.6699],\n",
       "        [ 0.0864,  2.6282],\n",
       "        [ 0.0853,  2.5858],\n",
       "        [ 0.0843,  2.5518],\n",
       "        [ 0.0803,  2.4401],\n",
       "        [ 0.0741,  2.3038],\n",
       "        [ 0.0672,  2.1798],\n",
       "        [ 0.0346,  1.7313],\n",
       "        [ 0.0183,  1.5423],\n",
       "        [ 0.1294,  0.3171],\n",
       "        [ 0.1333,  0.3634],\n",
       "        [ 0.1346,  0.3782],\n",
       "        [ 0.1382,  0.4255],\n",
       "        [ 0.1405,  0.4579],\n",
       "        [ 0.1447,  0.5222],\n",
       "        [ 0.1450,  0.5268],\n",
       "        [ 0.1495,  0.6098],\n",
       "        [ 0.1498,  0.6171],\n",
       "        [ 0.1537,  0.7158],\n",
       "        [ 0.1549,  0.7570],\n",
       "        [ 0.1564,  0.8368],\n",
       "        [ 0.1570,  0.8993],\n",
       "        [ 0.1568,  0.9683],\n",
       "        [ 0.1565,  1.0074],\n",
       "        [ 0.1538,  1.1267],\n",
       "        [ 0.1508,  1.2079],\n",
       "        [ 0.1443,  1.3315],\n",
       "        [ 0.1383,  1.4197],\n",
       "        [ 0.1324,  1.4949]], dtype=torch.float64)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsample_ang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, dataloader, device = 'cuda', epoch = int(1e5), num_bins = 100):\n",
    "    training_losses = []\n",
    "    for _ in tqdm(range(epoch)):\n",
    "        for batch in dataloader:\n",
    "            # parse the batch\n",
    "            ground_truth_distance = batch[:,0]\n",
    "            angles = batch[:,1:3]\n",
    "            origins = batch[:,3:6]\n",
    "            upsample_pos, upsample_ang, upsample_gt_distance = getUpSamples(origins, angles, ground_truth_distance, num_roll=10)\n",
    "            sample_pos, sample_ang, sample_org = getSamples(origins, angles, ground_truth_distance, num_bins=num_bins)\n",
    "            # tile distances\n",
    "            gt_distance_tiled = repeat(ground_truth_distance, 'b -> (b n) 1', n=num_bins)\n",
    "\n",
    "            # stack the upsampled position to sampled positions\n",
    "            pos = (torch.vstack((sample_pos, upsample_pos))).to(device)\n",
    "            ang = (torch.vstack((sample_ang, upsample_ang))).to(device)\n",
    "            gt_dis = (torch.vstack((gt_distance_tiled,upsample_gt_distance))).to(device)\n",
    "            org = (torch.vstack((sample_org, upsample_pos))).to(device)\n",
    "            \n",
    "            # inference and training\n",
    "            rendered_value = model(pos, ang)\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            rendered_value_sigmoid = sigmoid(rendered_value)\n",
    "            actual_value_sigmoided = (getTargetValues(pos, gt_dis, org)).to(dtype = torch.float32)\n",
    "            # loss = lossBCE(rendered_value, actual_value_sigmoided)  # + lossEikonal(model)\n",
    "            loss_bce = nn.BCELoss()\n",
    "            loss = loss_bce(rendered_value_sigmoid, actual_value_sigmoided)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_losses.append(loss.item())\n",
    "        scheduler.step()\n",
    "        print(loss.item())\n",
    "    return training_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "loaded data\n",
      "prepared data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]/tmp/ipykernel_10040/452579322.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dir_x = torch.tensor(np.cos(elev)*np.cos(pan))      # [batch_size]\n",
      "/tmp/ipykernel_10040/452579322.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dir_y = torch.tensor(np.cos(elev)*np.sin(pan))      # [batch_size]\n",
      "/tmp/ipykernel_10040/452579322.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dir_z = torch.tensor(np.sin(elev))\n",
      "/tmp/ipykernel_10040/452579322.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gt_tensor = torch.tensor(ground_truth_distance)\n",
      "/tmp/ipykernel_10040/452579322.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos = torch.tensor(origins_tiled) + multiplied\n",
      "/tmp/ipykernel_10040/452579322.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  angles_tiled = torch.tensor(repeat(angles, 'n c -> (n b) c', b = num_bins))\n",
      " 12%|█▎        | 1/8 [03:12<22:30, 192.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14833222329616547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [06:25<19:15, 192.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13577798008918762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [09:37<16:02, 192.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13381460309028625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [12:51<12:52, 193.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11695846915245056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [16:08<09:43, 194.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12252667546272278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [19:24<06:29, 194.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1169440895318985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [22:40<03:15, 195.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1229114830493927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [25:55<00:00, 194.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11599234491586685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")\n",
    "points = loadData()\n",
    "print(\"loaded data\")\n",
    "data_matrix = prepareData(points)\n",
    "print(\"prepared data\")\n",
    "training_dataset = torch.from_numpy(data_matrix)\n",
    "data_loader = DataLoader(training_dataset, batch_size=1024, shuffle = True)\n",
    "model = LiDAR_NeRF(hidden_dim=512, embedding_dim_dir=15, device = device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2, 4, 8, 16], gamma=0.5)\n",
    "losses = train(model, optimizer, scheduler, data_loader, epoch = 8, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Save the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal/models/version2_trial1.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "### Save the model\n",
    "torch.save(model.state_dict(), 'local/models/version2_trial1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load the model and try to \"visualize\" the model's datapoints\n",
    "model_evel = LiDAR_NeRF(hidden_dim=512, embedding_dim_dir=15, device = 'cpu')\n",
    "model_evel.load_state_dict(torch.load('local/models/version2_trial1.pth'))\n",
    "model_evel.eval(); # Set the model to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Render some structured pointcloud for evaluation\n",
    "with torch.no_grad():\n",
    "    dist = 0.1 # initial distanc forvisualization\n",
    "    pos = torch.zeros((100000,3))\n",
    "    ele = torch.linspace(-0.34, 0.3, 100)\n",
    "    pan = torch.linspace(-3.14, 3.14, 1000)\n",
    "    ele_tiled = repeat(ele, 'n -> (r n) 1', r = 1000)\n",
    "    pan_tiled = repeat(pan, 'n -> (n r) 1', r = 100)\n",
    "    ang = torch.cat((ele_tiled, pan_tiled), dim=1)\n",
    "\n",
    "    # direction for each \"point\" from camera centre\n",
    "    directions = torch.tensor(sph2cart(np.array(ang)))\n",
    "\n",
    "    for i in range(500):\n",
    "        output2 = model_evel(pos, ang)\n",
    "        temp = torch.sign(output2)\n",
    "        pos += directions * dist * temp\n",
    "        # dist /= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Render some structured pointcloud for evaluation\n",
    "with torch.no_grad():\n",
    "    dist = 32 # initial distanc forvisualization\n",
    "    pos = torch.zeros((100000,3))\n",
    "    pos[:,1] += 5\n",
    "    ele = torch.linspace(-0.34, 0.3, 100)\n",
    "    pan = torch.linspace(-3.14, 3.14, 1000)\n",
    "    ele_tiled = repeat(ele, 'n -> (r n) 1', r = 1000)\n",
    "    pan_tiled = repeat(pan, 'n -> (n r) 1', r = 100)\n",
    "    ang = torch.cat((ele_tiled, pan_tiled), dim=1)\n",
    "\n",
    "    # direction for each \"point\" from camera centre\n",
    "    directions = torch.tensor(sph2cart(np.array(ang)))\n",
    "\n",
    "    for i in range(10):\n",
    "        output2 = model_evel(pos, ang)\n",
    "        temp = torch.sign(output2)\n",
    "        pos += directions * dist * temp\n",
    "        dist /= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save to csv for visualization\n",
    "df_temp = pd.read_csv('local/visualize/dummy.csv')\n",
    "df_temp = df_temp.head(100000)\n",
    "pos_np = pos.numpy()\n",
    "df_temp['X'] = pos_np[:,0]\n",
    "df_temp['Y'] = pos_np[:,1]\n",
    "df_temp['Z'] = pos_np[:,2]\n",
    "df_temp.to_csv('local/visualize/visualize.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidar_nerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
