{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm       # for showing progress when training\n",
    "import pandas as pd         # for load data\n",
    "import numpy as np\n",
    "import open3d as o3d        # for getting point cloud register\n",
    "# import math as m            # for coordinate conversion\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from einops import rearrange, repeat\n",
    "#test update for branch wild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperation (NOTE: Using meter as unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "# convert pointcloud from cartisean coordinate to spherical coordinate\n",
    "def cart2sph(xyz):\n",
    "    x = xyz[:,0]\n",
    "    y = xyz[:,1]\n",
    "    z = xyz[:,2]\n",
    "    XsqPlusYsq = x**2 + y**2\n",
    "    r = np.sqrt(list(XsqPlusYsq + z**2))\n",
    "    elev = np.arctan2(list(z), np.sqrt(list(XsqPlusYsq)))\n",
    "    pan = np.arctan2(list(x), list(y))\n",
    "    output = np.array([r, elev, pan])\n",
    "    return rearrange(output, 'a b -> b a') #take transpose\n",
    "\n",
    "def sph2cart(ang):\n",
    "    ele = ang[:,0]\n",
    "    pan = ang[:,1]\n",
    "    x = np.cos(ele)*np.cos(pan)\n",
    "    y = np.cos(ele)*np.sin(pan)\n",
    "    z = np.sin(ele)\n",
    "    output = np.array([x,y,z])\n",
    "    return rearrange(output, 'a b -> b a') #take transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    # Specify the directory path\n",
    "    dataset_path = 'datasets/testing1'\n",
    "\n",
    "    # List all files in the specified path, ignoring directories\n",
    "    files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "    files.sort()\n",
    "\n",
    "    # read the files\n",
    "    points_xyz = []\n",
    "    for s in files:\n",
    "        path = 'datasets/testing1/' + s\n",
    "        df = pd.read_csv(path)\n",
    "        a = df.to_numpy()\n",
    "        points_xyz.append(a[:,8:11])\n",
    "    return points_xyz\n",
    "\n",
    "def getTransformation(data):\n",
    "    \"\"\" \n",
    "    Accept input of list of numpy array of n*3 size, \n",
    "    return list of 4*4 numpy array of transformation matrix\n",
    "    each are transformation needed from each frame to point cloud in fame 0\n",
    "    \"\"\"\n",
    "    # convert numpy array \n",
    "    point_clouds = []\n",
    "    for d in data:\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(d)\n",
    "        point_clouds.append(pcd)\n",
    "    threshold = 2.0 \n",
    "    trans_init = np.eye(4)  # Initial transformation matrix\n",
    "    transformations = []\n",
    "\n",
    "    # the following may take a while\n",
    "    for i in range(len(point_clouds)):\n",
    "        source_pcd = point_clouds[i]\n",
    "        target_pcd = point_clouds[0]\n",
    "        reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "                    source_pcd, \n",
    "                    target_pcd, \n",
    "                    threshold, \n",
    "                    trans_init,\n",
    "                    o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "        transformations.append(reg_p2p.transformation)\n",
    "    return transformations\n",
    "    \n",
    "def prepareData(points_xyz):\n",
    "    # get transformations\n",
    "    transformations = getTransformation(points_xyz)\n",
    "\n",
    "    # register all points onto same global coordinte (global coordinate align with frame 0 coordinate)\n",
    "    points_reg_xyz = []\n",
    "    for i, points in enumerate(points_xyz):\n",
    "        ones = np.ones((len(points), 1))\n",
    "        homo_points = np.hstack((points, ones))\n",
    "        # apply transformation\n",
    "        t = transformations[i]\n",
    "        t = rearrange(t, 'a b -> b a')\n",
    "        reg_points = homo_points@t\n",
    "        reg_points = reg_points / rearrange(reg_points[:,3], 'a -> a 1')\n",
    "        reg_points = reg_points[:,0:3]\n",
    "        points_reg_xyz.append(reg_points)\n",
    "\n",
    "    # create a list of origins \n",
    "    centres = [(t@np.array([[0],[0],[0],[1]]))[0:3,0] for t in transformations]\n",
    "    \n",
    "    # get the angular direction and distance of rays    \n",
    "    points_sph = []\n",
    "    for i, points in enumerate(points_reg_xyz):\n",
    "        relative_loc = points - centres[i]\n",
    "        points_sph.append(cart2sph(relative_loc))\n",
    "\n",
    "    # tile sensor centre \n",
    "    centres_tiled = []\n",
    "    for i, centre in enumerate(centres):\n",
    "        l = len(points_sph[i])\n",
    "        temp = np.tile(centre, (l, 1))\n",
    "        centres_tiled.append(temp)\n",
    "    \n",
    "    # stack everything into a matrix of size n * 6\n",
    "    # where n is number of points, 6 corrsponds to:\n",
    "    # distance, elevation, pan, x of camera, y of camera, z of camera\n",
    "    # stack the points into one big matrix\n",
    "    stacked = []\n",
    "    for i in range(len(points_sph)):\n",
    "        temp = np.hstack((points_sph[i], centres_tiled[i]))\n",
    "        stacked.append(temp)\n",
    "\n",
    "    dataset = np.array([])\n",
    "    for i in range(len(stacked)):\n",
    "        if i == 0:\n",
    "            dataset = stacked[i]\n",
    "        else:\n",
    "            dataset = np.vstack((dataset, stacked[i]))\n",
    "\n",
    "    # Mid pass filter, for distance value between 2 and 50 meter\n",
    "    mask1 = dataset[:,0] > 2\n",
    "    dataset = dataset[mask1]\n",
    "    mask2 = dataset[:,0] < 50\n",
    "    dataset = dataset[mask2]\n",
    "    np.random.shuffle(dataset)      # shuffle for good practice\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiDAR_NeRF(nn.Module):\n",
    "    def __init__(self, embedding_dim_pos = 10, embedding_dim_dir = 4, hidden_dim = 256, device = 'cuda'):\n",
    "        super(LiDAR_NeRF, self).__init__()\n",
    "        self.device = device\n",
    "        self.embedding_dim_dir = embedding_dim_dir\n",
    "        self.embedding_dim_pos = embedding_dim_pos\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim_pos * 6 + 3 + embedding_dim_dir * 4 + 2, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim_pos * 6 + 3 + embedding_dim_dir * 4 + 2 + hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def positional_encoding(x, L):\n",
    "        out = [x]\n",
    "        for j in range(L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    def forward(self, o, d):\n",
    "        emb_x = self.positional_encoding(o, self.embedding_dim_pos)\n",
    "        emb_d = self.positional_encoding(d, self.embedding_dim_dir)\n",
    "        input = torch.hstack((emb_x,emb_d)).to(dtype=torch.float32)\n",
    "        temp = self.block1(input)\n",
    "        input2 = torch.hstack((temp, input)).to(dtype=torch.float32) # add skip input\n",
    "        output = self.block2(input2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSamples(origins, angles, ground_truth_distance, num_bins = 100):\n",
    "    elev = angles[:,0]\n",
    "    pan = angles[:,1]\n",
    "    dir_x = torch.tensor(np.cos(elev)*np.cos(pan))      # [batch_size]\n",
    "    dir_y = torch.tensor(np.cos(elev)*np.sin(pan))      # [batch_size]\n",
    "    dir_z = torch.tensor(np.sin(elev))\n",
    "    gt_tensor = torch.tensor(ground_truth_distance)\n",
    "\n",
    "    # create a list of magnitudes with even spacing from 0 to 1\n",
    "    t = torch.linspace(0,1, num_bins).expand(dir_x.shape[0], num_bins)  # [batch_size, num_bins]\n",
    "    \n",
    "    # preterb the spacing\n",
    "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "    lower = torch.cat((t[:, :1], mid), -1)\n",
    "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape)\n",
    "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "    t = rearrange(t, 'a b -> b a')  # [num_bins, batch_size]  take transpose so that multiplication can broadcast\n",
    "    \n",
    "    # multiply the magnitude to ground truth distance and add 3 meter\n",
    "    t = torch.sqrt(t)\n",
    "    t = gt_tensor*t\n",
    "    t += 5\n",
    "\n",
    "    # convert magnitudes into positions by multiplying it to the unit vector\n",
    "    pos_x = dir_x*t     # [num_bins, batch_size]\n",
    "    pos_y = dir_y*t\n",
    "    pos_z = dir_z*t\n",
    "    # concat them for output\n",
    "    multiplied = rearrange([pos_x,pos_y,pos_z], 'c b n  -> (n b) c')   # [num_bin*batchsize, 3]\n",
    "    # tile the origin values\n",
    "    origins_tiled = repeat(origins, 'n c -> (n b) c', b = num_bins) # [num_bin*batch_size, 3]\n",
    "    pos = torch.tensor(origins_tiled) + multiplied\n",
    "    # tile the angle too\n",
    "    angles_tiled = torch.tensor(repeat(angles, 'n c -> (n b) c', b = num_bins))\n",
    "    return pos, angles_tiled, origins_tiled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUpSamples(origins, angles, gt_distance, num_rolls = 1):\n",
    "    upsample_pos = torch.empty(0,3)\n",
    "    upsample_ang = torch.empty(0,2)\n",
    "    upsample_gt_dist = torch.empty(0,1)\n",
    "\n",
    "    for num_roll in range(1, num_rolls+1):\n",
    "        # first, we prepare pairs of data, where one of them has a shorter ray, and another has a longer ray\n",
    "        gt_distance_rolled = torch.roll(gt_distance, num_roll, 0)\n",
    "        condition =  gt_distance < gt_distance_rolled\n",
    "        condition = rearrange(condition, 'a -> a 1')\n",
    "\n",
    "        dir = torch.tensor(sph2cart(angles))\n",
    "        gt_dist = rearrange(gt_distance, 'a -> a 1')\n",
    "        gt_distance_rolled = rearrange(gt_distance_rolled, 'a -> a 1')\n",
    "        pos = gt_dist * dir\n",
    "\n",
    "        pos_shorter = torch.where(condition, pos, torch.roll(pos, num_roll, 0))\n",
    "        origins_shorter = torch.where(condition, origins, torch.roll(origins, num_roll, 0))\n",
    "        angles_shorter = torch.where(condition, angles, torch.roll(angles, num_roll, 0))\n",
    "        gt_dist_shorter = torch.where(condition, gt_dist, gt_distance_rolled)\n",
    "        pos_longer = torch.where(condition, torch.roll(pos, num_roll, 0), pos)\n",
    "        origins_longer = torch.where(condition, torch.roll(origins, num_roll, 0), origins)\n",
    "        angles_longer = torch.where(condition, torch.roll(angles, num_roll, 0), angles)\n",
    "        gt_dist_longer = torch.where(condition, gt_distance_rolled, gt_dist)\n",
    "        \n",
    "        # check if angle between pairs are small\n",
    "        diff = torch.abs(angles_shorter - angles_longer)\n",
    "        mask_small_ang = (diff < 0.09).all(dim=1)   ### NOTE: hardcoded 0.09 radient difference max\n",
    "\n",
    "        # check if origin between pairs are small\n",
    "        diff2 = torch.abs(origins_shorter - origins_longer)\n",
    "        mask_small_org = (diff2 < 0.2).all(dim=1)   ### pass if coordinate in origin are less than 20cm in all dimensions\n",
    "        \n",
    "        # # check if the target location is close\n",
    "        # diff3 = torch.abs(pos_shorter - pos_longer)\n",
    "        # mask_small_dis = (diff3 < 0.2).all(dim=1)\n",
    "\n",
    "        # get masks for both cases\n",
    "        mask_same_org = mask_small_ang & mask_small_org\n",
    "        # mask_same_tag = mask_small_ang & mask_small_dis\n",
    "\n",
    "        ### Handling case of same origin\n",
    "        # prepare set where rays are to be upsampled \n",
    "        # ensuring samples are at rays that are longer\n",
    "        angles_from_same_org = angles_longer[mask_same_org]\n",
    "        origins_from_same_org = origins_longer[mask_same_org]\n",
    "        gt_dist_to_same_org = gt_dist_shorter[mask_same_org] \n",
    "        pos_to_same_org = pos_shorter[mask_same_org]\n",
    "\n",
    "        if angles_from_same_org.shape[0] == 0:\n",
    "            continue   # skip if there are no points available for upsampling\n",
    "\n",
    "        # calculate upsampling position\n",
    "        num_bins = 20 \n",
    "        t = torch.linspace(0,1, num_bins).expand(angles_from_same_org.shape[0], num_bins)  # [batch_size, num_bins]\n",
    "        \n",
    "        # preterb the spacing\n",
    "        mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "        lower = torch.cat((t[:, :1], mid), -1)\n",
    "        upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "        u = torch.rand(t.shape)\n",
    "        t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "        t = rearrange(t, 'a b -> b a')  # [num_bins, batch_size]  take transpose so that multiplication can broadcast\n",
    "        t = rearrange(t, 'a b -> (b a) 1')\n",
    "        t = torch.sqrt(t)\n",
    "        \n",
    "        # get the sampling positions\n",
    "        origins_from_tiled = repeat(origins_from_same_org, 'n c -> (n b) c', b = num_bins)\n",
    "        dir_from = torch.tensor(sph2cart(angles_from_same_org))\n",
    "        dir_from_tiled = repeat(dir_from, 'n c -> (n b) c', b = num_bins)\n",
    "        gt_dist_to_tiled = repeat(gt_dist_to_same_org, 'n c -> (n b) c', b = num_bins)     \n",
    "\n",
    "        pos_from = origins_from_tiled + dir_from_tiled * t * gt_dist_to_tiled\n",
    "        pos_to_tiled = repeat(pos_to_same_org, 'n c -> (n b) c', b = num_bins)\n",
    "\n",
    "        # also calculte the ground truth distance of our up sampled location\n",
    "        sample_sph = cart2sph(pos_from - pos_to_tiled)\n",
    "        sample_direction = torch.tensor(sample_sph[:,1:])\n",
    "        sample_gt_distance = torch.tensor(sample_sph[:,0])\n",
    "\n",
    "        # add one more dimension to sample_gt_distance\n",
    "        sample_gt_distance = rearrange(sample_gt_distance, 'a -> a 1')\n",
    "\n",
    "        upsample_pos = torch.vstack((upsample_pos, pos_from))\n",
    "        upsample_ang = torch.vstack((upsample_ang, sample_direction))\n",
    "        upsample_gt_dist = torch.vstack((upsample_gt_dist, sample_gt_distance))\n",
    "\n",
    "    # return pos_from , torch.tensor(sample_direction), torch.tensor(sample_gt_distance)\n",
    "    return upsample_pos, upsample_ang, upsample_gt_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns pytorch tensor of sigmoid of projected SDF\n",
    "def getTargetValues(sample_positions, gt_distance, origins, num_bins=100):\n",
    "    # calculate distance from sample_position\n",
    "    temp = torch.tensor((sample_positions - origins)**2)\n",
    "    pos_distance = torch.sqrt(torch.sum(temp, dim=1, keepdim=True))\n",
    "    \n",
    "    # find the \"projected\" value\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    values = sigmoid(-(pos_distance - gt_distance))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # constants\n",
    "# num_bins = 100\n",
    "# device = 'cuda'\n",
    "\n",
    "# # sample data for testing\n",
    "# points = loadData()\n",
    "# dataset = prepareData(points)\n",
    "# test_batch = torch.tensor(dataset[0:256,:])\n",
    "# gt_distance = test_batch[:,0]\n",
    "# angles = test_batch[:,1:3]\n",
    "# origins = test_batch[:,3:6]\n",
    "# upsample_pos, upsample_ang, upsample_gt_distance = getUpSamples(origins, angles,  gt_distance, num_rolls=5)\n",
    "\n",
    "\n",
    "# sample_pos, sample_ang, sample_org = getSamples(origins, angles, gt_distance, num_bins = num_bins)\n",
    "# gt_distance_tiled = repeat(gt_distance, 'b -> (b n) 1', n = num_bins)\n",
    "\n",
    "# # stack the upsampled position to sampled positions\n",
    "# pos = (torch.vstack((sample_pos, upsample_pos))).to(device)\n",
    "# ang = (torch.vstack((sample_ang, upsample_ang))).to(device)\n",
    "# gt_dis = (torch.vstack((gt_distance_tiled,upsample_gt_distance) )).to(device)\n",
    "# org = (torch.vstack((sample_org, upsample_pos))).to(device)\n",
    "\n",
    "# # inference and training\n",
    "# model = LiDAR_NeRF(hidden_dim=512).to(device)\n",
    "# rendered_value = model(pos, ang)\n",
    "# sigmoid = nn.Sigmoid()\n",
    "# rendered_value_sigmoided = sigmoid(rendered_value)\n",
    "# actual_value_sigmoided = (getTargetValues(pos, gt_dis, org)).to(dtype = torch.float32)\n",
    "# # loss = lossBCE(rendered_value, actual_value_sigmoided)  # + lossEikonal(model)\n",
    "# loss_bce = nn.BCELoss()\n",
    "# loss = loss_bce(rendered_value_sigmoided, actual_value_sigmoided)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, dataloader, device = 'cuda', epoch = int(1e5), num_bins = 100):\n",
    "    training_losses = []\n",
    "    for _ in tqdm(range(epoch)):\n",
    "        for batch in dataloader:\n",
    "            # parse the batch\n",
    "            ground_truth_distance = batch[:,0]\n",
    "            angles = batch[:,1:3]\n",
    "            origins = batch[:,3:6]\n",
    "            upsample_pos, upsample_ang, upsample_gt_distance = getUpSamples(origins, angles, ground_truth_distance, num_rolls=0)\n",
    "            sample_pos, sample_ang, sample_org = getSamples(origins, angles, ground_truth_distance, num_bins=num_bins)\n",
    "            # tile distances\n",
    "            gt_distance_tiled = repeat(ground_truth_distance, 'b -> (b n) 1', n=num_bins)\n",
    "\n",
    "            # stack the upsampled position to sampled positions\n",
    "            pos = (torch.vstack((sample_pos, upsample_pos))).to(device)\n",
    "            ang = (torch.vstack((sample_ang, upsample_ang))).to(device)\n",
    "            gt_dis = (torch.vstack((gt_distance_tiled,upsample_gt_distance))).to(device)\n",
    "            org = (torch.vstack((sample_org, upsample_pos))).to(device)\n",
    "            \n",
    "            # inference\n",
    "            rendered_value = model(pos, ang)\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            rendered_value_sigmoid = sigmoid(rendered_value)\n",
    "            actual_value_sigmoided = (getTargetValues(pos, gt_dis, org)).to(dtype = torch.float32)\n",
    "\n",
    "            # loss = lossBCE(rendered_value, actual_value_sigmoided)  # + lossEikonal(model)\n",
    "            # back propergate\n",
    "            loss_bce = nn.BCELoss()\n",
    "            loss = loss_bce(rendered_value_sigmoid, actual_value_sigmoided)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_losses.append(loss.item())\n",
    "        scheduler.step()\n",
    "        print(loss.item())\n",
    "    return training_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "loaded data\n",
      "prepared data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]/tmp/ipykernel_32038/1749373479.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dir_x = torch.tensor(np.cos(elev)*np.cos(pan))      # [batch_size]\n",
      "/tmp/ipykernel_32038/1749373479.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dir_y = torch.tensor(np.cos(elev)*np.sin(pan))      # [batch_size]\n",
      "/tmp/ipykernel_32038/1749373479.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dir_z = torch.tensor(np.sin(elev))\n",
      "/tmp/ipykernel_32038/1749373479.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gt_tensor = torch.tensor(ground_truth_distance)\n",
      "/tmp/ipykernel_32038/1749373479.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos = torch.tensor(origins_tiled) + multiplied\n",
      "/tmp/ipykernel_32038/1749373479.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  angles_tiled = torch.tensor(repeat(angles, 'n c -> (n b) c', b = num_bins))\n",
      " 12%|█▎        | 1/8 [05:32<38:48, 332.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2858654260635376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [11:00<32:57, 329.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28038448095321655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [16:26<27:20, 328.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2816518247127533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [21:50<21:46, 326.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27802497148513794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [27:19<16:21, 327.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27063432335853577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [32:47<10:55, 327.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2841636538505554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [38:11<05:26, 326.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26745888590812683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [43:34<00:00, 326.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2780597507953644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")\n",
    "points = loadData()\n",
    "print(\"loaded data\")\n",
    "data_matrix = prepareData(points)\n",
    "print(\"prepared data\")\n",
    "training_dataset = torch.from_numpy(data_matrix)\n",
    "data_loader = DataLoader(training_dataset, batch_size=512, shuffle = True)\n",
    "model = LiDAR_NeRF(hidden_dim=512, embedding_dim_dir=15, device = device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2, 4, 8, 16], gamma=0.5)\n",
    "losses = train(model, optimizer, scheduler, data_loader, epoch = 8, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model\n",
    "torch.save(model.state_dict(), 'local/models/version2_trial4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load the model and try to \"visualize\" the model's datapoints\n",
    "model_evel = LiDAR_NeRF(hidden_dim=512, embedding_dim_dir=15, device = 'cpu')\n",
    "model_evel.load_state_dict(torch.load('local/models/version2_trial4.pth'))\n",
    "model_evel.eval(); # Set the model to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Render some structured pointcloud for evaluation\n",
    "with torch.no_grad():\n",
    "    dist = 0.1 # initial distanc forvisualization\n",
    "    pos = torch.zeros((100000,3))\n",
    "    ele = torch.linspace(-0.34, 0.3, 100)\n",
    "    pan = torch.linspace(-3.14, 3.14, 1000)\n",
    "    ele_tiled = repeat(ele, 'n -> (r n) 1', r = 1000)\n",
    "    pan_tiled = repeat(pan, 'n -> (n r) 1', r = 100)\n",
    "    ang = torch.cat((ele_tiled, pan_tiled), dim=1)\n",
    "\n",
    "    # direction for each \"point\" from camera centre\n",
    "    directions = torch.tensor(sph2cart(np.array(ang)))\n",
    "\n",
    "    for i in range(500):\n",
    "        output2 = model_evel(pos, ang)\n",
    "        temp = torch.sign(output2)\n",
    "        pos += directions * dist * temp\n",
    "        # dist /= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Render some structured pointcloud for evaluation\n",
    "with torch.no_grad():\n",
    "    dist = 32 # initial distanc forvisualization\n",
    "    pos = torch.zeros((100000,3))\n",
    "    pos[:,1] += 2\n",
    "    ele = torch.linspace(-0.34, 0.3, 100)\n",
    "    pan = torch.linspace(-3.14, 3.14, 1000)\n",
    "    ele_tiled = repeat(ele, 'n -> (r n) 1', r = 1000)\n",
    "    pan_tiled = repeat(pan, 'n -> (n r) 1', r = 100)\n",
    "    ang = torch.cat((ele_tiled, pan_tiled), dim=1)\n",
    "\n",
    "    # direction for each \"point\" from camera centre\n",
    "    directions = torch.tensor(sph2cart(np.array(ang)))\n",
    "\n",
    "    for i in range(10):\n",
    "        output2 = model_evel(pos, ang)\n",
    "        temp = torch.sign(output2)\n",
    "        pos += directions * dist * temp\n",
    "        dist /= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save to csv for visualization\n",
    "df_temp = pd.read_csv('local/visualize/dummy.csv')\n",
    "df_temp = df_temp.head(100000)\n",
    "pos_np = pos.numpy()\n",
    "df_temp['X'] = pos_np[:,0]\n",
    "df_temp['Y'] = pos_np[:,1]\n",
    "df_temp['Z'] = pos_np[:,2]\n",
    "df_temp.to_csv('local/visualize/visualize.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidar_nerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
