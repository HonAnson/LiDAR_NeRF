{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "from einops import rearrange, repeat\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperation (NOTE: Using meter as unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "# convert pointcloud from cartisean coordinate to spherical coordinate\n",
    "def cart2sph(xyz):\n",
    "    x = xyz[:,0]\n",
    "    y = xyz[:,1]\n",
    "    z = xyz[:,2]\n",
    "    XsqPlusYsq = x**2 + y**2\n",
    "    r = np.sqrt(list(XsqPlusYsq + z**2))\n",
    "    elev = np.arctan2(list(z), np.sqrt(list(XsqPlusYsq)))\n",
    "    pan = np.arctan2(list(x), list(y))\n",
    "\n",
    "    output = np.array([r, elev, pan])\n",
    "    return rearrange(output, 'a b -> b a') #take transpose\n",
    "\n",
    "\n",
    "def sph2cart(ang):\n",
    "    ele = ang[:,0]\n",
    "    pan = ang[:,1]\n",
    "    x = np.cos(ele)*np.cos(pan)\n",
    "    y = np.cos(ele)*np.sin(pan)\n",
    "    z = np.sin(ele)\n",
    "    output = np.array([x,y,z])\n",
    "    return rearrange(output, 'a b -> b a') #take transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    # Specify the directory path\n",
    "    dataset_path = 'datasets/testing1'\n",
    "\n",
    "    # List all files in the specified path, ignoring directories\n",
    "    files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "    files.sort()\n",
    "\n",
    "    # read the files\n",
    "    points_xyz = []\n",
    "    for s in files:\n",
    "        path = 'datasets/testing1/' + s\n",
    "        df = pd.read_csv(path)\n",
    "        a = df.to_numpy()\n",
    "        points_xyz.append(a[:,8:11])\n",
    "    return points_xyz\n",
    "\n",
    "def prepareData(points_xyz):\n",
    "    # Find the fiew direction of each points:\n",
    "    # NOTE: points in spherical coordinate are arranged: [r, elev, pan]\n",
    "    points_sphere = []\n",
    "    for points in points_xyz:\n",
    "        points_sphere.append(cart2sph(points))\n",
    "\n",
    "    ### Process the data\n",
    "    # Translation vectors for points in each view, we are using camera centre at first frame as origin of world coordinate\n",
    "    # NOTE: translation vectors below are found by assuming transformation between frames are translations, and obatined by manually finding corrspondance\n",
    "    # They are translation of the same corrspondance across different frames\n",
    "    # HARD CODED HERE\n",
    "    t0 = np.array([0,0,0])\n",
    "    t1 = np.array([-0.671,-0.016,0.215])\n",
    "    t2 = np.array([-1.825,-0.091,0.147])\n",
    "    t3 = np.array([-2.661,-0.263,0.166])\n",
    "    t4 = np.array([-3.607,-0.156,0.039])\n",
    "    translations = [t0, t1, t2, t3, t4]\n",
    "\n",
    "    # camera centre locations\n",
    "    centres = [-t for t in translations]\n",
    "    centres_data = []\n",
    "    for i,c in enumerate(centres):\n",
    "        l = len(points_sphere[i])\n",
    "        temp = np.tile(c, (l, 1))\n",
    "        centres_data.append(temp)\n",
    "\n",
    "    # stack the points into one big matrix\n",
    "    stacked = []\n",
    "    for i in range(len(points_sphere)):\n",
    "        temp = np.hstack((points_sphere[i], centres_data[i]))\n",
    "        stacked.append(temp)\n",
    "\n",
    "    dataset = np.array([])\n",
    "    for i in range(len(stacked)):\n",
    "        if i == 0:\n",
    "            dataset = stacked[i]\n",
    "        else:\n",
    "            dataset = np.vstack((dataset, stacked[i]))\n",
    "    np.random.shuffle(dataset)\n",
    "\n",
    "    # Mid pass filter, for distance value between 2 and 50 meter\n",
    "    mask1 = dataset[:,0] > 2\n",
    "    dataset = dataset[mask1]\n",
    "    mask2 = dataset[:,0] < 50\n",
    "    dataset = dataset[mask2]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiDAR_NeRF(nn.Module):\n",
    "    def __init__(self, embedding_dim_pos = 10, embedding_dim_dir = 4, hidden_dim = 256, device = 'cuda'):\n",
    "        super(LiDAR_NeRF, self).__init__()\n",
    "        self.device = device\n",
    "        self.embedding_dim_dir = embedding_dim_dir\n",
    "        self.embedding_dim_pos = embedding_dim_pos\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim_pos * 6 + 3 + embedding_dim_dir * 4 + 2, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim_pos * 6 + 3 + embedding_dim_dir * 4 + 2 + hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),               \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def positional_encoding(x, L):\n",
    "        out = [x]\n",
    "        for j in range(L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "    \n",
    "\n",
    "    def forward(self, o, d):\n",
    "        emb_x = self.positional_encoding(o, self.embedding_dim_pos)\n",
    "        emb_d = self.positional_encoding(d, self.embedding_dim_dir)\n",
    "        input = torch.hstack((emb_x,emb_d)).to(dtype=torch.float32)\n",
    "        temp = self.block1(input)\n",
    "        input2 = torch.hstack((temp, input)).to(dtype=torch.float32) # add skip input\n",
    "        output = self.block2(input2)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_positions(origins, angles, ground_truth_distance, num_bins = 100, device = 'cpu'):\n",
    "    elev = angles[:,0]\n",
    "    pan = angles[:,1]\n",
    "    dir_x = torch.tensor(np.cos(elev)*np.cos(pan))      # [batch_size]\n",
    "    dir_y = torch.tensor(np.cos(elev)*np.sin(pan))      # [batch_size]\n",
    "    dir_z = torch.tensor(np.sin(elev))\n",
    "    gt_tensor = torch.tensor(ground_truth_distance)\n",
    "    # create a list of magnitudes with even spacing from 0 to 1\n",
    "    t = torch.linspace(0,1, num_bins, device=device).expand(dir_x.shape[0], num_bins)  # [batch_size, num_bins]\n",
    "    \n",
    "    # preterb the spacing\n",
    "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "    lower = torch.cat((t[:, :1], mid), -1)\n",
    "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape, device = device)\n",
    "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "    t = rearrange(t, 'a b -> b a')  # [num_bins, batch_size]  take transpose so that multiplication can broadcast\n",
    "\n",
    "    # multiply the magnitude to ground truth distance and add 3 meter\n",
    "    t = torch.sqrt(t)\n",
    "    t = gt_tensor*t\n",
    "    t += 10\n",
    "\n",
    "    # convert magnitudes into positions by multiplying it to the unit vector\n",
    "    pos_x = dir_x*t     # [num_bins, batch_size]\n",
    "    pos_y = dir_y*t\n",
    "    pos_z = dir_z*t\n",
    "    # concat them for output\n",
    "    multiplied = rearrange([pos_x,pos_y,pos_z], 'c b n  -> (n b) c')   # [num_bin*batchsize, 3]\n",
    "    # tile the origin values\n",
    "    origins_tiled = repeat(origins, 'n c -> (n b) c', b = num_bins) # [num_bin*batch_size, 3]\n",
    "    pos = torch.tensor(origins_tiled) + multiplied\n",
    "    # tile the angle too\n",
    "    angles_tiled = torch.tensor(repeat(angles, 'n c -> (n b) c', b = num_bins))\n",
    "    return pos, angles_tiled, origins_tiled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns pytorch tensor of sigmoid of projected SDF\n",
    "def get_actual_value(sample_positions, gt_distance, origins, num_bins=100):\n",
    "    # calculate distance from sample_position\n",
    "    temp = torch.tensor((sample_positions)**2)\n",
    "    pos_distance = torch.sqrt(torch.sum(temp, dim=1, keepdim=True))\n",
    "    # tile distances\n",
    "    gt_distance_tiled = repeat(gt_distance, 'b -> (b n) 1', n=num_bins)\n",
    "    # find the \"projected\" value\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    values = sigmoid(-(pos_distance - gt_distance_tiled))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13509/3230770518.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  temp = torch.tensor((sample_positions)**2)\n"
     ]
    }
   ],
   "source": [
    "# sample data for testing\n",
    "points = loadData()\n",
    "dataset = prepareData(points)\n",
    "test_batch = dataset[128:256,:]\n",
    "ground_truth_distance = test_batch[:,0]\n",
    "angles = test_batch[:,1:3]\n",
    "origin = test_batch[:,3:6]\n",
    "pos, ang, origins = get_sample_positions(origin, angles, ground_truth_distance ,num_bins=100)\n",
    "val = (get_actual_value(pos, ground_truth_distance, origins)).to(dtype = torch.float32)\n",
    "\n",
    "model = LiDAR_NeRF(hidden_dim=256)\n",
    "rendered = model(pos, ang)\n",
    "sigmoid = nn.Sigmoid()\n",
    "rendered_sigmoid = sigmoid(rendered)\n",
    "temp = torch.zeros_like(pos)\n",
    "\n",
    "# for x in val:\n",
    "#     print(x)\n",
    "# print(min(rendered))\n",
    "# loss_bce = nn.BCELoss()\n",
    "# loss = loss_bce(rendered_sigmoid, val)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, dataloader, device = 'cpu', epoch = int(1e5), num_bins = 100):\n",
    "    training_losses = []\n",
    "    for _ in tqdm(range(epoch)):\n",
    "        for batch in dataloader:\n",
    "            # parse the batch\n",
    "            ground_truth_distance = batch[:,0]\n",
    "            angles = batch[:,1:3]\n",
    "            origin = batch[:,3:6]\n",
    "            \n",
    "            sample_positions, sample_angles, sample_origins = get_sample_positions(origin, angles, ground_truth_distance, num_bins=num_bins)\n",
    "            rendered_value = model(sample_positions.to(device), sample_angles.to(device))\n",
    "            \n",
    "            sigmoid = nn.Sigmoid()\n",
    "            rendered_value_sigmoid = sigmoid(rendered_value)\n",
    "            actual_value_sigmoided = (get_actual_value(sample_positions.to(device), ground_truth_distance.to(device), sample_origins.to(device))).to(dtype = torch.float32)\n",
    "            # print(rendered_value_sigmoid[0:10]) \n",
    "            # loss = lossBCE(rendered_value, actual_value_sigmoided)  # + lossEikonal(model)\n",
    "            loss_bce = nn.BCELoss()\n",
    "            loss = loss_bce(rendered_value_sigmoid, actual_value_sigmoided)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_losses.append(loss.item())\n",
    "        scheduler.step()\n",
    "        print(loss.item())\n",
    "    return training_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "loaded data\n",
      "prepared data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]/tmp/ipykernel_32347/2524055478.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dir_x = torch.tensor(np.cos(elev)*np.cos(pan))      # [batch_size]\n",
      "/tmp/ipykernel_32347/2524055478.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dir_y = torch.tensor(np.cos(elev)*np.sin(pan))      # [batch_size]\n",
      "/tmp/ipykernel_32347/2524055478.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dir_z = torch.tensor(np.sin(elev))\n",
      "/tmp/ipykernel_32347/2524055478.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gt_tensor = torch.tensor(ground_truth_distance)\n",
      "/tmp/ipykernel_32347/2524055478.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos = torch.tensor(origins_tiled) + multiplied\n",
      "/tmp/ipykernel_32347/2524055478.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  angles_tiled = torch.tensor(repeat(angles, 'n c -> (n b) c', b = num_bins))\n",
      " 12%|█▎        | 1/8 [03:09<22:03, 189.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14784710109233856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [06:20<19:02, 190.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13408119976520538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [09:33<15:58, 191.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13046765327453613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [12:48<12:52, 193.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13021467626094818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [16:03<09:40, 193.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12028110027313232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [19:18<06:28, 194.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11808963119983673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [22:32<03:14, 194.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12474925816059113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [25:46<00:00, 193.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1224590465426445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")\n",
    "points = loadData()\n",
    "print(\"loaded data\")\n",
    "data_matrix = prepareData(points)\n",
    "print(\"prepared data\")\n",
    "training_dataset = torch.from_numpy(data_matrix)\n",
    "data_loader = DataLoader(training_dataset, batch_size=1024, shuffle = True)\n",
    "model = LiDAR_NeRF(hidden_dim=512, embedding_dim_dir=10, device = device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2, 4, 8, 16], gamma=0.5)\n",
    "losses = train(model, optimizer, scheduler, data_loader, epoch = 8, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model\n",
    "torch.save(model.state_dict(), 'local/models/version1_trial8.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load the model and try to \"visualize\" the model's datapoints\n",
    "model_evel = LiDAR_NeRF(hidden_dim=512, embedding_dim_dir=10, device = 'cpu')\n",
    "model_evel.load_state_dict(torch.load('local/models/version1_trial4.pth'))\n",
    "model_evel.eval(); # Set the model to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample data for testing\n",
    "# points = loadData()\n",
    "# dataset = prepareData(points)\n",
    "# test_batch = dataset[0:512,:]\n",
    "# ground_truth_distance = test_batch[:,0]\n",
    "# angles = test_batch[:,1:3]\n",
    "# origin = test_batch[:,3:]\n",
    "# pos, ang = get_sample_positions(origin, angles, ground_truth_distance, num_bins=100)\n",
    "# # pos = torch.zeros_like(pos)\n",
    "# val = (get_actual_value(pos, ground_truth_distance)).to(dtype = torch.float32)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     pos_tensor = torch.tensor(pos)\n",
    "#     ang_tensor = torch.tensor(ang)\n",
    "#     output = model_evel(pos_tensor, ang_tensor)\n",
    "\n",
    "# sig = nn.Sigmoid()\n",
    "# output_sigmoided = sig(output)\n",
    "# lossBCE = nn.BCELoss()\n",
    "# loss = lossBCE(val, output_sigmoided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Render some structured pointcloud for evaluation\n",
    "with torch.no_grad():\n",
    "    dist = 0.1 # initial distanc forvisualization\n",
    "    pos = torch.zeros((100000,3))\n",
    "    ele = torch.linspace(-0.34, 0.3, 100)\n",
    "    pan = torch.linspace(-3.14, 3.14, 1000)\n",
    "    ele_tiled = repeat(ele, 'n -> (r n) 1', r = 1000)\n",
    "    pan_tiled = repeat(pan, 'n -> (n r) 1', r = 100)\n",
    "    ang = torch.cat((ele_tiled, pan_tiled), dim=1)\n",
    "\n",
    "    # direction for each \"point\" from camera centre\n",
    "    directions = torch.tensor(sph2cart(np.array(ang)))\n",
    "\n",
    "    for i in range(500):\n",
    "        output2 = model_evel(pos, ang)\n",
    "        temp = torch.sign(output2)\n",
    "        pos += directions * dist * temp\n",
    "        # dist /= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Render some structured pointcloud for evaluation\n",
    "with torch.no_grad():\n",
    "    dist = 32 # initial distanc forvisualization\n",
    "    pos = torch.zeros((100000,3))\n",
    "    ele = torch.linspace(-0.34, 0.3, 100)\n",
    "    pan = torch.linspace(-3.14, 3.14, 1000)\n",
    "    ele_tiled = repeat(ele, 'n -> (r n) 1', r = 1000)\n",
    "    pan_tiled = repeat(pan, 'n -> (n r) 1', r = 100)\n",
    "    ang = torch.cat((ele_tiled, pan_tiled), dim=1)\n",
    "\n",
    "    # direction for each \"point\" from camera centre\n",
    "    directions = torch.tensor(sph2cart(np.array(ang)))\n",
    "\n",
    "    for i in range(10):\n",
    "        output2 = model_evel(pos, ang)\n",
    "        temp = torch.sign(output2)\n",
    "        pos += directions * dist * temp\n",
    "        dist /= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = loadData()\n",
    "data = prepareData(points)\n",
    "ang = data[:,1:3]\n",
    "dir = sph2cart(ang)\n",
    "r = rearrange(data[:,0], 'a -> a 1')\n",
    "pos_np = dir*r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save to csv for visualization\n",
    "df_temp = pd.read_csv('local/visualize/dummy.csv')\n",
    "df_temp = df_temp.head(554952)\n",
    "# pos_np = pos.numpy()\n",
    "\n",
    "df_temp['X'] = pos_np[:,0]\n",
    "df_temp['Y'] = pos_np[:,1]\n",
    "df_temp['Z'] = pos_np[:,2]\n",
    "df_temp.to_csv('local/visualize/register_check2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidar_nerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
